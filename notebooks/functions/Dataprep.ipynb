{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.2.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.2.min.js\", \"https://cdn.holoviz.org/panel/1.2.3/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy as cart; import cartopy.crs as ccrs\n",
    "import matplotlib as mpl\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import pathlib as pl\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "from datetime import datetime\n",
    "import matplotlib.font_manager as fm\n",
    "import nbformat\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot\n",
    "import statsmodels.api as sm\n",
    "import plotly.express as px # for data visualization\n",
    "import matplotlib.cm as cm # for color mapping\n",
    "import geopandas as gpd\n",
    "import shutil\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import LineString\n",
    "from pyproj import Geod\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from scipy.interpolate import interp1d # for interpolation of new data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371 # radius of Earth in km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === USER INPUTS ===\n",
    "input_folder = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\nasa\\Utqi\\0129')  # Folder with ICESat-2 .h5 files\n",
    "coastline_path = pl.Path(r'C:\\coding\\arctic\\Gis\\datasets\\MyOwn\\AOI_shoreline.shp')  # Polyline shapefile\n",
    "output_folder = input_folder / \"filtered2\"  # Output folder\n",
    "beam_groups = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "buffer_dist = 500  # meters\n",
    "\n",
    "# === PREP ===\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "coastline = gpd.read_file(coastline_path).to_crs(\"EPSG:3413\")\n",
    "coast_buffer_union = gpd.GeoSeries(coastline.buffer(buffer_dist).union_all(), crs=coastline.crs)\n",
    "\n",
    "# === PROCESS EACH .H5 FILE ===\n",
    "for h5_file in input_folder.glob(\"*.h5\"):\n",
    "    print(f\"üìÇ Processing: {h5_file.name}\")\n",
    "    try:\n",
    "        for beam in beam_groups:\n",
    "            try:\n",
    "                ds = xr.open_dataset(h5_file, group=f'/{beam}/land_ice_segments', engine='h5netcdf')\n",
    "\n",
    "                # Extract lat/lon/h_li\n",
    "                lat = ds['latitude'].values\n",
    "                lon = ds['longitude'].values\n",
    "                h_li = ds['h_li'].values\n",
    "\n",
    "                # Extract date and track_id from filename\n",
    "                parts = h5_file.stem.split('_')\n",
    "                datetime_str = parts[2]        # '20190105212430'\n",
    "                track_info = parts[3]          # '01290203'\n",
    "                date = datetime_str[:8]        # '20190105'\n",
    "                track_id = track_info[:4]      # '0129'\n",
    "\n",
    "                # Calculate the distance between each point\n",
    "                distance = np.zeros(lat.shape)\n",
    "                for i in range(1, len(lat)):\n",
    "                    distance[i] = haversine_distance(lat[0], lon[0], lat[i], lon[i])\n",
    "                distance = distance * 1000  # convert to meters\n",
    "\n",
    "                # Create GeoDataFrame\n",
    "                df = pd.DataFrame({\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'h_li': h_li,\n",
    "                    'distance': distance,\n",
    "                    'track_id': track_id,\n",
    "                    'gt': beam,\n",
    "                    'date': date\n",
    "                })\n",
    "                gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=\"EPSG:4326\")\n",
    "                gdf = gdf.to_crs(\"EPSG:3413\")\n",
    "\n",
    "                # Clip to buffer\n",
    "                selected = gdf[gdf.geometry.within(coast_buffer_union[0])]\n",
    "\n",
    "                if not selected.empty:\n",
    "                    out_name = f\"ATL06_{track_id}_{beam}_{date}.shp\"\n",
    "                    selected.to_file(output_folder / out_name)\n",
    "                    print(f\"‚úÖ {beam}: {len(selected)} points saved.\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è {beam}: No points within buffer.\")\n",
    "\n",
    "            except Exception as beam_error:\n",
    "                print(f\"‚ö†Ô∏è Skipping beam {beam} in {h5_file.name}: {beam_error}\")\n",
    "\n",
    "    except Exception as file_error:\n",
    "        print(f\"‚ùå Failed to process {h5_file.name}: {file_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === USER INPUTS ===\n",
    "input_folder = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\nasa\\0312') # Folder with ICESat-2 .h5 files\n",
    "coastline_path = pl.Path(r'C:\\coding\\arctic\\Gis\\datasets\\MyOwn\\AOI_shoreline.shp')  # Polyline shapefile\n",
    "output_folder = input_folder / \"filtered2\"  # Output folder\n",
    "beam_groups = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "buffer_dist = 500  # meters\n",
    "\n",
    "# === PREP ===\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "coastline = gpd.read_file(coastline_path).to_crs(\"EPSG:3413\")\n",
    "coast_buffer_union = gpd.GeoSeries(coastline.buffer(buffer_dist).union_all(), crs=coastline.crs)\n",
    "\n",
    "# === PROCESS EACH .H5 FILE ===\n",
    "for h5_file in input_folder.glob(\"*.h5\"):\n",
    "    print(f\"üìÇ Processing: {h5_file.name}\")\n",
    "    try:\n",
    "        for beam in beam_groups:\n",
    "            try:\n",
    "                ds = xr.open_dataset(h5_file, group=f'/{beam}/land_ice_segments', engine='h5netcdf')\n",
    "\n",
    "                # Extract lat/lon/h_li\n",
    "                lat = ds['latitude'].values\n",
    "                lon = ds['longitude'].values\n",
    "                h_li = ds['h_li'].values\n",
    "\n",
    "                # Extract date and track_id from filename\n",
    "                parts = h5_file.stem.split('_')\n",
    "                datetime_str = parts[2]        # '20190105212430'\n",
    "                track_info = parts[3]          # '01290203'\n",
    "                date = datetime_str[:8]        # '20190105'\n",
    "                track_id = track_info[:4]      # '0129'\n",
    "\n",
    "                # Calculate the distance between each point\n",
    "                distance = np.zeros(lat.shape)\n",
    "                for i in range(1, len(lat)):\n",
    "                    distance[i] = haversine_distance(lat[0], lon[0], lat[i], lon[i])\n",
    "                distance = distance * 1000  # convert to meters\n",
    "\n",
    "                # Create GeoDataFrame\n",
    "                df = pd.DataFrame({\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'h_li': h_li,\n",
    "                    'distance': distance,\n",
    "                    'track_id': track_id,\n",
    "                    'gt': beam,\n",
    "                    'date': date\n",
    "                })\n",
    "                gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=\"EPSG:4326\")\n",
    "                gdf = gdf.to_crs(\"EPSG:3413\")\n",
    "\n",
    "                # Clip to buffer\n",
    "                selected = gdf[gdf.geometry.within(coast_buffer_union[0])]\n",
    "\n",
    "                if not selected.empty:\n",
    "                    out_name = f\"ATL06_{track_id}_{beam}_{date}.shp\"\n",
    "                    selected.to_file(output_folder / out_name)\n",
    "                    print(f\"‚úÖ {beam}: {len(selected)} points saved.\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è {beam}: No points within buffer.\")\n",
    "\n",
    "            except Exception as beam_error:\n",
    "                print(f\"‚ö†Ô∏è Skipping beam {beam} in {h5_file.name}: {beam_error}\")\n",
    "\n",
    "    except Exception as file_error:\n",
    "        print(f\"‚ùå Failed to process {h5_file.name}: {file_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === USER INPUTS ===\n",
    "input_folder = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\nasa\\0114')  # Folder with ICESat-2 .h5 files\n",
    "coastline_path = pl.Path(r'C:\\coding\\arctic\\paper1\\shp\\AK_northslope_Project.shp')  # Polyline shapefile\n",
    "output_folder = input_folder / \"filtered\"  # Output folder\n",
    "beam_groups = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "buffer_dist = 500  # meters\n",
    "\n",
    "# === PREP ===\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "coastline = gpd.read_file(coastline_path).to_crs(\"EPSG:3413\")\n",
    "coast_buffer_union = gpd.GeoSeries(coastline.buffer(buffer_dist).union_all(), crs=coastline.crs)\n",
    "\n",
    "# === PROCESS EACH .H5 FILE ===\n",
    "for h5_file in input_folder.glob(\"*.h5\"):\n",
    "    print(f\"üìÇ Processing: {h5_file.name}\")\n",
    "    try:\n",
    "        for beam in beam_groups:\n",
    "            try:\n",
    "                ds = xr.open_dataset(h5_file, group=f'/{beam}/land_ice_segments', engine='h5netcdf')\n",
    "\n",
    "                # Extract lat/lon/h_li\n",
    "                lat = ds['latitude'].values\n",
    "                lon = ds['longitude'].values\n",
    "                h_li = ds['h_li'].values\n",
    "\n",
    "                # Extract date and track_id from filename\n",
    "                parts = h5_file.stem.split('_')\n",
    "                datetime_str = parts[2]        # '20190105212430'\n",
    "                track_info = parts[3]          # '01290203'\n",
    "                date = datetime_str[:8]        # '20190105'\n",
    "                track_id = track_info[:4]      # '0129'\n",
    "\n",
    "                # Calculate the distance between each point\n",
    "                distance = np.zeros(lat.shape)\n",
    "                for i in range(1, len(lat)):\n",
    "                    distance[i] = haversine_distance(lat[0], lon[0], lat[i], lon[i])\n",
    "                distance = distance * 1000  # convert to meters\n",
    "\n",
    "                # Create GeoDataFrame\n",
    "                df = pd.DataFrame({\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'h_li': h_li,\n",
    "                    'distance': distance,\n",
    "                    'track_id': track_id,\n",
    "                    'gt': beam,\n",
    "                    'date': date\n",
    "                })\n",
    "                gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=\"EPSG:4326\")\n",
    "                gdf = gdf.to_crs(\"EPSG:3413\")\n",
    "\n",
    "                # Clip to buffer\n",
    "                selected = gdf[gdf.geometry.within(coast_buffer_union[0])]\n",
    "\n",
    "                if not selected.empty:\n",
    "                    out_name = f\"ATL06_{track_id}_{beam}_{date}.shp\"\n",
    "                    selected.to_file(output_folder / out_name)\n",
    "                    print(f\"‚úÖ {beam}: {len(selected)} points saved.\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è {beam}: No points within buffer.\")\n",
    "\n",
    "            except Exception as beam_error:\n",
    "                print(f\"‚ö†Ô∏è Skipping beam {beam} in {h5_file.name}: {beam_error}\")\n",
    "\n",
    "    except Exception as file_error:\n",
    "        print(f\"‚ùå Failed to process {h5_file.name}: {file_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === USER INPUTS ===\n",
    "input_folder = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\nasa\\0114')  # Folder with ICESat-2 .h5 files\n",
    "coastline_path = pl.Path(r'C:\\coding\\arctic\\paper1\\shp\\AK_northslope_Project.shp')  # Polyline shapefile\n",
    "output_folder = input_folder / \"filtered\"  # Output folder\n",
    "beam_groups = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "buffer_dist = 500  # meters\n",
    "\n",
    "# === PREP ===\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "coastline = gpd.read_file(coastline_path).to_crs(\"EPSG:3413\")\n",
    "coast_buffer_union = gpd.GeoSeries(coastline.buffer(buffer_dist).union_all(), crs=coastline.crs)\n",
    "\n",
    "# === PROCESS EACH .H5 FILE ===\n",
    "for h5_file in input_folder.glob(\"*.h5\"):\n",
    "    print(f\"üìÇ Processing: {h5_file.name}\")\n",
    "    try:\n",
    "        for beam in beam_groups:\n",
    "            try:\n",
    "                ds = xr.open_dataset(h5_file, group=f'/{beam}/land_ice_segments', engine='h5netcdf')\n",
    "\n",
    "                # Extract lat/lon/h_li\n",
    "                lat = ds['latitude'].values\n",
    "                lon = ds['longitude'].values\n",
    "                h_li = ds['h_li'].values\n",
    "\n",
    "                # Extract date and track_id from filename\n",
    "                parts = h5_file.stem.split('_')\n",
    "                datetime_str = parts[2]        # '20190105212430'\n",
    "                track_info = parts[3]          # '01290203'\n",
    "                date = datetime_str[:8]        # '20190105'\n",
    "                track_id = track_info[:4]      # '0129'\n",
    "\n",
    "                # Calculate the distance between each point\n",
    "                distance = np.zeros(lat.shape)\n",
    "                for i in range(1, len(lat)):\n",
    "                    distance[i] = haversine_distance(lat[0], lon[0], lat[i], lon[i])\n",
    "                distance = distance * 1000  # convert to meters\n",
    "\n",
    "                # Create GeoDataFrame\n",
    "                df = pd.DataFrame({\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'h_li': h_li,\n",
    "                    'distance': distance,\n",
    "                    'track_id': track_id,\n",
    "                    'gt': beam,\n",
    "                    'date': date\n",
    "                })\n",
    "                gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=\"EPSG:4326\")\n",
    "                gdf = gdf.to_crs(\"EPSG:3413\")\n",
    "\n",
    "                # Clip to buffer\n",
    "                selected = gdf[gdf.geometry.within(coast_buffer_union[0])]\n",
    "\n",
    "                if not selected.empty:\n",
    "                    out_name = f\"ATL06_{track_id}_{beam}_{date}.shp\"\n",
    "                    selected.to_file(output_folder / out_name)\n",
    "                    print(f\"‚úÖ {beam}: {len(selected)} points saved.\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è {beam}: No points within buffer.\")\n",
    "\n",
    "            except Exception as beam_error:\n",
    "                print(f\"‚ö†Ô∏è Skipping beam {beam} in {h5_file.name}: {beam_error}\")\n",
    "\n",
    "    except Exception as file_error:\n",
    "        print(f\"‚ùå Failed to process {h5_file.name}: {file_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371 # radius of Earth in km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Files Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Processing: ATL06_20190301072956_09600205_007_01.h5\n",
      "  ‚úÖ gt1l: 1132 pts ‚Üí ATL06_0960_gt1l_20190301.shp\n",
      "  ‚úÖ gt1r: 1137 pts ‚Üí ATL06_0960_gt1r_20190301.shp\n",
      "  ‚úÖ gt2l: 754 pts ‚Üí ATL06_0960_gt2l_20190301.shp\n",
      "  ‚úÖ gt2r: 748 pts ‚Üí ATL06_0960_gt2r_20190301.shp\n",
      "  ‚úÖ gt3l: 220 pts ‚Üí ATL06_0960_gt3l_20190301.shp\n",
      "  ‚úÖ gt3r: 220 pts ‚Üí ATL06_0960_gt3r_20190301.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20190531030930_09600305_007_01.h5\n",
      "  ‚úÖ gt1l: 34 pts ‚Üí ATL06_0960_gt1l_20190531.shp\n",
      "  ‚ö†Ô∏è  gt1r: no points within 2000 m buffer.\n",
      "  ‚úÖ gt2l: 32 pts ‚Üí ATL06_0960_gt2l_20190531.shp\n",
      "  ‚úÖ gt2r: 3 pts ‚Üí ATL06_0960_gt2r_20190531.shp\n",
      "  ‚ö†Ô∏è  gt3l: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt3r: no points within 2000 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20190829224919_09600405_007_01.h5\n",
      "  ‚úÖ gt1l: 549 pts ‚Üí ATL06_0960_gt1l_20190829.shp\n",
      "  ‚úÖ gt1r: 427 pts ‚Üí ATL06_0960_gt1r_20190829.shp\n",
      "  ‚úÖ gt2l: 207 pts ‚Üí ATL06_0960_gt2l_20190829.shp\n",
      "  ‚úÖ gt2r: 214 pts ‚Üí ATL06_0960_gt2r_20190829.shp\n",
      "  ‚úÖ gt3l: 215 pts ‚Üí ATL06_0960_gt3l_20190829.shp\n",
      "  ‚úÖ gt3r: 211 pts ‚Üí ATL06_0960_gt3r_20190829.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20191128182910_09600505_007_01.h5\n",
      "  ‚úÖ gt1l: 24 pts ‚Üí ATL06_0960_gt1l_20191128.shp\n",
      "  ‚úÖ gt1r: 60 pts ‚Üí ATL06_0960_gt1r_20191128.shp\n",
      "  ‚ö†Ô∏è  gt2l: no points within 2000 m buffer.\n",
      "  ‚úÖ gt2r: 27 pts ‚Üí ATL06_0960_gt2r_20191128.shp\n",
      "  ‚úÖ gt3l: 17 pts ‚Üí ATL06_0960_gt3l_20191128.shp\n",
      "  ‚úÖ gt3r: 54 pts ‚Üí ATL06_0960_gt3r_20191128.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20200227140854_09600605_007_01.h5\n",
      "  ‚úÖ gt1l: 988 pts ‚Üí ATL06_0960_gt1l_20200227.shp\n",
      "  ‚úÖ gt1r: 964 pts ‚Üí ATL06_0960_gt1r_20200227.shp\n",
      "  ‚úÖ gt2l: 430 pts ‚Üí ATL06_0960_gt2l_20200227.shp\n",
      "  ‚úÖ gt2r: 414 pts ‚Üí ATL06_0960_gt2r_20200227.shp\n",
      "  ‚úÖ gt3l: 215 pts ‚Üí ATL06_0960_gt3l_20200227.shp\n",
      "  ‚úÖ gt3r: 216 pts ‚Üí ATL06_0960_gt3r_20200227.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20200528094844_09600705_007_01.h5\n",
      "  ‚úÖ gt1l: 769 pts ‚Üí ATL06_0960_gt1l_20200528.shp\n",
      "  ‚úÖ gt1r: 739 pts ‚Üí ATL06_0960_gt1r_20200528.shp\n",
      "  ‚úÖ gt2l: 237 pts ‚Üí ATL06_0960_gt2l_20200528.shp\n",
      "  ‚úÖ gt2r: 211 pts ‚Üí ATL06_0960_gt2r_20200528.shp\n",
      "  ‚úÖ gt3l: 4 pts ‚Üí ATL06_0960_gt3l_20200528.shp\n",
      "  ‚ö†Ô∏è  gt3r: no points within 2000 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20200827052830_09600805_007_01.h5\n",
      "  ‚ö†Ô∏è  gt1l: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt1r: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt2l: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt2r: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt3l: no points within 2000 m buffer.\n",
      "  ‚úÖ gt3r: 1 pts ‚Üí ATL06_0960_gt3r_20200827.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20201126010820_09600905_007_01.h5\n",
      "  ‚úÖ gt1l: 988 pts ‚Üí ATL06_0960_gt1l_20201126.shp\n",
      "  ‚úÖ gt1r: 964 pts ‚Üí ATL06_0960_gt1r_20201126.shp\n",
      "  ‚úÖ gt2l: 432 pts ‚Üí ATL06_0960_gt2l_20201126.shp\n",
      "  ‚úÖ gt2r: 414 pts ‚Üí ATL06_0960_gt2r_20201126.shp\n",
      "  ‚úÖ gt3l: 215 pts ‚Üí ATL06_0960_gt3l_20201126.shp\n",
      "  ‚úÖ gt3r: 216 pts ‚Üí ATL06_0960_gt3r_20201126.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20210224204813_09601005_007_01.h5\n",
      "  ‚úÖ gt1l: 988 pts ‚Üí ATL06_0960_gt1l_20210224.shp\n",
      "  ‚úÖ gt1r: 964 pts ‚Üí ATL06_0960_gt1r_20210224.shp\n",
      "  ‚úÖ gt2l: 430 pts ‚Üí ATL06_0960_gt2l_20210224.shp\n",
      "  ‚úÖ gt2r: 412 pts ‚Üí ATL06_0960_gt2r_20210224.shp\n",
      "  ‚úÖ gt3l: 215 pts ‚Üí ATL06_0960_gt3l_20210224.shp\n",
      "  ‚úÖ gt3r: 216 pts ‚Üí ATL06_0960_gt3r_20210224.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20210526162804_09601105_007_01.h5\n",
      "  ‚ö†Ô∏è  gt1l: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt1r: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt2l: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt2r: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt3l: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt3r: no points within 2000 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20210825120758_09601205_007_01.h5\n",
      "  ‚úÖ gt1l: 250 pts ‚Üí ATL06_0960_gt1l_20210825.shp\n",
      "  ‚úÖ gt1r: 365 pts ‚Üí ATL06_0960_gt1r_20210825.shp\n",
      "  ‚úÖ gt2l: 123 pts ‚Üí ATL06_0960_gt2l_20210825.shp\n",
      "  ‚úÖ gt2r: 193 pts ‚Üí ATL06_0960_gt2r_20210825.shp\n",
      "  ‚ö†Ô∏è  gt3l: no points within 2000 m buffer.\n",
      "  ‚úÖ gt3r: 8 pts ‚Üí ATL06_0960_gt3r_20210825.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20211124074757_09601305_007_01.h5\n",
      "  ‚úÖ gt1l: 987 pts ‚Üí ATL06_0960_gt1l_20211124.shp\n",
      "  ‚úÖ gt1r: 964 pts ‚Üí ATL06_0960_gt1r_20211124.shp\n",
      "  ‚úÖ gt2l: 432 pts ‚Üí ATL06_0960_gt2l_20211124.shp\n",
      "  ‚úÖ gt2r: 412 pts ‚Üí ATL06_0960_gt2r_20211124.shp\n",
      "  ‚úÖ gt3l: 215 pts ‚Üí ATL06_0960_gt3l_20211124.shp\n",
      "  ‚úÖ gt3r: 216 pts ‚Üí ATL06_0960_gt3r_20211124.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20220223032746_09601405_007_01.h5\n",
      "  ‚úÖ gt1l: 4 pts ‚Üí ATL06_0960_gt1l_20220223.shp\n",
      "  ‚ö†Ô∏è  gt1r: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt2l: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt2r: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt3l: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt3r: no points within 2000 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20220524230736_09601505_007_01.h5\n",
      "  ‚úÖ gt1l: 127 pts ‚Üí ATL06_0960_gt1l_20220524.shp\n",
      "  ‚úÖ gt1r: 12 pts ‚Üí ATL06_0960_gt1r_20220524.shp\n",
      "  ‚úÖ gt2l: 155 pts ‚Üí ATL06_0960_gt2l_20220524.shp\n",
      "  ‚úÖ gt2r: 45 pts ‚Üí ATL06_0960_gt2r_20220524.shp\n",
      "  ‚úÖ gt3l: 135 pts ‚Üí ATL06_0960_gt3l_20220524.shp\n",
      "  ‚úÖ gt3r: 64 pts ‚Üí ATL06_0960_gt3r_20220524.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20220823184739_09601605_007_01.h5\n",
      "  ‚ö†Ô∏è  gt1l: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt1r: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt2l: no points within 2000 m buffer.\n",
      "  ‚úÖ gt2r: 13 pts ‚Üí ATL06_0960_gt2r_20220823.shp\n",
      "  ‚ö†Ô∏è  gt3l: no points within 2000 m buffer.\n",
      "  ‚ö†Ô∏è  gt3r: no points within 2000 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20221122142717_09601705_007_01.h5\n",
      "  ‚úÖ gt1l: 781 pts ‚Üí ATL06_0960_gt1l_20221122.shp\n",
      "  ‚úÖ gt1r: 952 pts ‚Üí ATL06_0960_gt1r_20221122.shp\n",
      "  ‚úÖ gt2l: 254 pts ‚Üí ATL06_0960_gt2l_20221122.shp\n",
      "  ‚úÖ gt2r: 391 pts ‚Üí ATL06_0960_gt2r_20221122.shp\n",
      "  ‚úÖ gt3l: 192 pts ‚Üí ATL06_0960_gt3l_20221122.shp\n",
      "  ‚úÖ gt3r: 215 pts ‚Üí ATL06_0960_gt3r_20221122.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20230221100711_09601805_007_01.h5\n",
      "  ‚úÖ gt1l: 980 pts ‚Üí ATL06_0960_gt1l_20230221.shp\n",
      "  ‚úÖ gt1r: 958 pts ‚Üí ATL06_0960_gt1r_20230221.shp\n",
      "  ‚úÖ gt2l: 430 pts ‚Üí ATL06_0960_gt2l_20230221.shp\n",
      "  ‚úÖ gt2r: 411 pts ‚Üí ATL06_0960_gt2r_20230221.shp\n",
      "  ‚úÖ gt3l: 215 pts ‚Üí ATL06_0960_gt3l_20230221.shp\n",
      "  ‚úÖ gt3r: 216 pts ‚Üí ATL06_0960_gt3r_20230221.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20230523054646_09601905_007_01.h5\n",
      "  ‚úÖ gt1l: 982 pts ‚Üí ATL06_0960_gt1l_20230523.shp\n",
      "  ‚úÖ gt1r: 958 pts ‚Üí ATL06_0960_gt1r_20230523.shp\n",
      "  ‚úÖ gt2l: 430 pts ‚Üí ATL06_0960_gt2l_20230523.shp\n",
      "  ‚úÖ gt2r: 412 pts ‚Üí ATL06_0960_gt2r_20230523.shp\n",
      "  ‚úÖ gt3l: 215 pts ‚Üí ATL06_0960_gt3l_20230523.shp\n",
      "  ‚úÖ gt3r: 216 pts ‚Üí ATL06_0960_gt3r_20230523.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20230822012553_09602005_007_01.h5\n",
      "  ‚úÖ gt1l: 773 pts ‚Üí ATL06_0960_gt1l_20230822.shp\n",
      "  ‚úÖ gt1r: 727 pts ‚Üí ATL06_0960_gt1r_20230822.shp\n",
      "  ‚úÖ gt2l: 232 pts ‚Üí ATL06_0960_gt2l_20230822.shp\n",
      "  ‚úÖ gt2r: 191 pts ‚Üí ATL06_0960_gt2r_20230822.shp\n",
      "  ‚úÖ gt3l: 94 pts ‚Üí ATL06_0960_gt3l_20230822.shp\n",
      "  ‚úÖ gt3r: 73 pts ‚Üí ATL06_0960_gt3r_20230822.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20231120210543_09602105_007_01.h5\n",
      "  ‚úÖ gt1l: 304 pts ‚Üí ATL06_0960_gt1l_20231120.shp\n",
      "  ‚úÖ gt1r: 596 pts ‚Üí ATL06_0960_gt1r_20231120.shp\n",
      "  ‚úÖ gt2l: 118 pts ‚Üí ATL06_0960_gt2l_20231120.shp\n",
      "  ‚úÖ gt2r: 332 pts ‚Üí ATL06_0960_gt2r_20231120.shp\n",
      "  ‚úÖ gt3l: 63 pts ‚Üí ATL06_0960_gt3l_20231120.shp\n",
      "  ‚úÖ gt3r: 212 pts ‚Üí ATL06_0960_gt3r_20231120.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20240219164503_09602205_007_01.h5\n",
      "  ‚úÖ gt1l: 982 pts ‚Üí ATL06_0960_gt1l_20240219.shp\n",
      "  ‚úÖ gt1r: 958 pts ‚Üí ATL06_0960_gt1r_20240219.shp\n",
      "  ‚úÖ gt2l: 430 pts ‚Üí ATL06_0960_gt2l_20240219.shp\n",
      "  ‚úÖ gt2r: 410 pts ‚Üí ATL06_0960_gt2r_20240219.shp\n",
      "  ‚úÖ gt3l: 215 pts ‚Üí ATL06_0960_gt3l_20240219.shp\n",
      "  ‚úÖ gt3r: 216 pts ‚Üí ATL06_0960_gt3r_20240219.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20241118034407_09602505_007_01.h5\n",
      "  ‚úÖ gt1l: 870 pts ‚Üí ATL06_0960_gt1l_20241118.shp\n",
      "  ‚úÖ gt1r: 800 pts ‚Üí ATL06_0960_gt1r_20241118.shp\n",
      "  ‚úÖ gt2l: 171 pts ‚Üí ATL06_0960_gt2l_20241118.shp\n",
      "  ‚úÖ gt2r: 84 pts ‚Üí ATL06_0960_gt2r_20241118.shp\n",
      "  ‚úÖ gt3l: 44 pts ‚Üí ATL06_0960_gt3l_20241118.shp\n",
      "  ‚úÖ gt3r: 3 pts ‚Üí ATL06_0960_gt3r_20241118.shp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === USER INPUTS ===\n",
    "input_folder = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\andre\\0960\\track_0960')  # Folder with ICESat-2 .h5 files\n",
    "coastline_path = pl.Path(r'C:\\coding\\arctic\\Gis\\datasets\\MyOwn\\AOI_shoreline.shp')  # Polyline shapefile\n",
    "output_folder = input_folder / \"filtered\"  # Output folder\n",
    "beam_groups = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "buffer_dist = 2000  # meters\n",
    "\n",
    "WRITE_GPKG = False         # set True if you want GeoPackage too\n",
    "WRITE_GEOPARQUET = False    # modern + fast format\n",
    "\n",
    "# === HELPERS ===\n",
    "GEOD = Geod(ellps=\"WGS84\")\n",
    "\n",
    "def cumdist_geodesic(lon, lat):\n",
    "    \"\"\"\n",
    "    Cumulative geodesic distance (meters) along the given lon/lat sequence.\n",
    "    Assumes the sequence is already ordered the way you want (e.g., N->S).\n",
    "    \"\"\"\n",
    "    lon = np.asarray(lon, dtype=float)\n",
    "    lat = np.asarray(lat, dtype=float)\n",
    "    n = len(lon)\n",
    "    if n == 0:\n",
    "        return np.array([], dtype=float)\n",
    "    if n == 1:\n",
    "        return np.array([0.0], dtype=float)\n",
    "    # pairwise distances\n",
    "    _, _, d = GEOD.inv(lon[:-1], lat[:-1], lon[1:], lat[1:])\n",
    "    return np.concatenate(([0.0], np.cumsum(d)))\n",
    "\n",
    "def safe_first(arr):\n",
    "    try:\n",
    "        return arr[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def parse_date_track_from_name(h5_path):\n",
    "    \"\"\"\n",
    "    Parse YYYYMMDD and 4-digit track_id from ATL06 filenames like:\n",
    "    ATL06_20190105212430_01290203_006_01.h5\n",
    "                ^^^^^^^^  ^^^^\n",
    "    Returns (date_str, track_id) or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    stem = Path(h5_path).stem\n",
    "    parts = stem.split('_')\n",
    "\n",
    "    date = None\n",
    "    track_id = None\n",
    "\n",
    "    # Primary: strict per your rule\n",
    "    if len(parts) >= 3:\n",
    "        # parts[1] = 'YYYYMMDDHHMMSS' ‚Üí take first 8\n",
    "        if parts[1].isdigit() and len(parts[1]) >= 8:\n",
    "            date = parts[1][:8]\n",
    "        # parts[2] = '01290203' ‚Üí take first 4 as track\n",
    "        if parts[2].isdigit() and len(parts[2]) >= 4:\n",
    "            track_id = parts[2][:4]\n",
    "\n",
    "    # Fallback: regex (handles minor naming variations)\n",
    "    if date is None or track_id is None:\n",
    "        m = re.search(r'_(\\d{8})(?:\\d{6})?_([0-9]{4})', stem)\n",
    "        if m:\n",
    "            date = date or m.group(1)\n",
    "            track_id = track_id or m.group(2)\n",
    "\n",
    "    return date, track_id\n",
    "\n",
    "# === PREP ===\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Read coastline and build a single buffer polygon in meters CRS (EPSG:3413)\n",
    "coastline = gpd.read_file(coastline_path).to_crs(\"EPSG:3413\")\n",
    "# If the shoreline layer has multiple parts, .buffer() then unary_union gives one geometry\n",
    "coast_buffer_geom = coastline.buffer(buffer_dist).union_all()  # shapely (Multi)Polygon\n",
    "\n",
    "# === PROCESS EACH .H5 FILE ===\n",
    "for h5_file in sorted(input_folder.glob(\"*.h5\")):\n",
    "    print(f\"\\nüìÇ Processing: {h5_file.name}\")\n",
    "    try:\n",
    "        for beam in beam_groups:\n",
    "            try:\n",
    "                group = f'/{beam}/land_ice_segments'\n",
    "                # Open as context to ensure clean close\n",
    "                with xr.open_dataset(h5_file, group=group, engine='h5netcdf') as ds:\n",
    "                    # Required vars (drop NaNs right away)\n",
    "                    if not all(v in ds.variables for v in ['latitude', 'longitude', 'h_li']):\n",
    "                        print(f\"  ‚ö†Ô∏è  {beam}: missing required vars; skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    lat = ds['latitude'].values\n",
    "                    lon = ds['longitude'].values\n",
    "                    h_li = ds['h_li'].values\n",
    "\n",
    "                    # Quality filter (optional; comment out if you don't want it)\n",
    "                    # Keep 'good' segments only if available\n",
    "                    if 'atl06_quality_summary' in ds.variables:\n",
    "                        q = ds['atl06_quality_summary'].values\n",
    "                        good = (q == 0)\n",
    "                        lat, lon, h_li = lat[good], lon[good], h_li[good]\n",
    "\n",
    "                    # Drop NaNs\n",
    "                    m = np.isfinite(lat) & np.isfinite(lon) & np.isfinite(h_li)\n",
    "                    lat, lon, h_li = lat[m], lon[m], h_li[m]\n",
    "                    if lat.size == 0:\n",
    "                        print(f\"  ‚ö†Ô∏è  {beam}: no valid points after QC/NaN filter.\")\n",
    "                        continue\n",
    "\n",
    "                    # Metadata: date & track (prefer variables, fallback to filename)\n",
    "                    date_str, track_id_name = parse_date_track_from_name(h5_file)\n",
    "\n",
    "                    # RGT and cycle (if present)\n",
    "                    rgt = None\n",
    "                    cycle = None\n",
    "                    if 'rgt' in ds.variables:\n",
    "                        try:\n",
    "                            rgt = int(np.nanmedian(ds['rgt'].values))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    if 'cycle_number' in ds.variables:\n",
    "                        try:\n",
    "                            cycle = int(np.nanmedian(ds['cycle_number'].values))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    # Fallbacks\n",
    "                    if track_id_name is None and rgt is not None:\n",
    "                        track_id_name = f\"{rgt:04d}\"\n",
    "\n",
    "                    # Build GeoDataFrame in lon/lat, then project to meters CRS for clipping\n",
    "                    gdf = gpd.GeoDataFrame(\n",
    "                        {\n",
    "                            'latitude': lat,\n",
    "                            'longitude': lon,\n",
    "                            'h_li': h_li,\n",
    "                            'track_id': track_id_name,\n",
    "                            'gt': beam,\n",
    "                            'date': date_str,\n",
    "                            'rgt': rgt,\n",
    "                            'cycle': cycle,\n",
    "                        },\n",
    "                        geometry=gpd.points_from_xy(lon, lat),\n",
    "                        crs=\"EPSG:4326\",\n",
    "                    ).to_crs(\"EPSG:3413\")\n",
    "\n",
    "                    # --- EARLY CLIP TO BUFFER (includes boundary) ---\n",
    "                    # For points, 'intersects' behaves like \"inside or on boundary\"\n",
    "                    in_buf = gdf.geometry.intersects(coast_buffer_geom)\n",
    "                    selected = gdf.loc[in_buf].copy()\n",
    "\n",
    "                    if selected.empty:\n",
    "                        print(f\"  ‚ö†Ô∏è  {beam}: no points within {buffer_dist} m buffer.\")\n",
    "                        continue\n",
    "\n",
    "                    # Restore lon/lat columns after projection (keep both CRSes if you like)\n",
    "                    selected_ll = selected.to_crs(\"EPSG:4326\")\n",
    "                    selected['latitude'] = selected_ll.geometry.y.values\n",
    "                    selected['longitude'] = selected_ll.geometry.x.values\n",
    "\n",
    "                    # --- ORDER & DISTANCE: start at northernmost point ---\n",
    "                    selected.sort_values('latitude', ascending=False, inplace=True)\n",
    "                    # Use geodesic cumulative distance along lon/lat (meters)\n",
    "                    dists = cumdist_geodesic(selected['longitude'].values,\n",
    "                                             selected['latitude'].values)\n",
    "                    selected['distance_m'] = dists\n",
    "\n",
    "                    # --- WRITE OUTPUTS ---\n",
    "                    out_stem = f\"ATL06_{selected['track_id'].iloc[0] or 'unk'}_{beam}_{date_str or 'nodate'}\"\n",
    "                    shp_path = output_folder / f\"{out_stem}.shp\"\n",
    "                    selected.to_file(shp_path)\n",
    "                    print(f\"  ‚úÖ {beam}: {len(selected)} pts ‚Üí {shp_path.name}\")\n",
    "\n",
    "                    if WRITE_GPKG:\n",
    "                        gpkg_path = output_folder / f\"{out_stem}.gpkg\"\n",
    "                        selected.to_file(gpkg_path, driver=\"GPKG\")\n",
    "                    if WRITE_GEOPARQUET:\n",
    "                        parquet_path = output_folder / f\"{out_stem}.parquet\"\n",
    "                        selected.to_parquet(parquet_path, index=False)\n",
    "\n",
    "            except Exception as beam_error:\n",
    "                print(f\"  ‚ö†Ô∏è  Skipping {beam} in {h5_file.name}: {beam_error}\")\n",
    "\n",
    "    except Exception as file_error:\n",
    "        print(f\"‚ùå Failed to process {h5_file.name}: {file_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Processing: ATL06_20190531030930_09600305_006_02.h5\n",
      "  ‚úÖ gt1l: 3 pts ‚Üí ATL06_0960_gt1l_20190531.shp\n",
      "  ‚ö†Ô∏è  gt1r: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt2l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt2r: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt3l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt3r: no points within 500 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20190829224919_09600405_006_02.h5\n",
      "  ‚úÖ gt1l: 150 pts ‚Üí ATL06_0960_gt1l_20190829.shp\n",
      "  ‚úÖ gt1r: 94 pts ‚Üí ATL06_0960_gt1r_20190829.shp\n",
      "  ‚úÖ gt2l: 50 pts ‚Üí ATL06_0960_gt2l_20190829.shp\n",
      "  ‚úÖ gt2r: 49 pts ‚Üí ATL06_0960_gt2r_20190829.shp\n",
      "  ‚úÖ gt3l: 54 pts ‚Üí ATL06_0960_gt3l_20190829.shp\n",
      "  ‚úÖ gt3r: 52 pts ‚Üí ATL06_0960_gt3r_20190829.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20191128182910_09600505_006_01.h5\n",
      "  ‚úÖ gt1l: 23 pts ‚Üí ATL06_0960_gt1l_20191128.shp\n",
      "  ‚úÖ gt1r: 47 pts ‚Üí ATL06_0960_gt1r_20191128.shp\n",
      "  ‚ö†Ô∏è  gt2l: no points within 500 m buffer.\n",
      "  ‚úÖ gt2r: 5 pts ‚Üí ATL06_0960_gt2r_20191128.shp\n",
      "  ‚úÖ gt3l: 11 pts ‚Üí ATL06_0960_gt3l_20191128.shp\n",
      "  ‚úÖ gt3r: 25 pts ‚Üí ATL06_0960_gt3r_20191128.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20200227140854_09600605_006_01.h5\n",
      "  ‚úÖ gt1l: 240 pts ‚Üí ATL06_0960_gt1l_20200227.shp\n",
      "  ‚úÖ gt1r: 233 pts ‚Üí ATL06_0960_gt1r_20200227.shp\n",
      "  ‚úÖ gt2l: 50 pts ‚Üí ATL06_0960_gt2l_20200227.shp\n",
      "  ‚úÖ gt2r: 50 pts ‚Üí ATL06_0960_gt2r_20200227.shp\n",
      "  ‚úÖ gt3l: 54 pts ‚Üí ATL06_0960_gt3l_20200227.shp\n",
      "  ‚úÖ gt3r: 52 pts ‚Üí ATL06_0960_gt3r_20200227.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20200528094844_09600705_006_01.h5\n",
      "  ‚úÖ gt1l: 185 pts ‚Üí ATL06_0960_gt1l_20200528.shp\n",
      "  ‚úÖ gt1r: 178 pts ‚Üí ATL06_0960_gt1r_20200528.shp\n",
      "  ‚ö†Ô∏è  gt2l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt2r: no points within 500 m buffer.\n",
      "  ‚úÖ gt3l: 3 pts ‚Üí ATL06_0960_gt3l_20200528.shp\n",
      "  ‚ö†Ô∏è  gt3r: no points within 500 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20200827052830_09600805_006_01.h5\n",
      "  ‚ö†Ô∏è  gt1l: no points within 500 m buffer.\n",
      "  ‚úÖ gt1r: 1 pts ‚Üí ATL06_0960_gt1r_20200827.shp\n",
      "  ‚ö†Ô∏è  gt2l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt2r: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt3l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt3r: no points within 500 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20201126010820_09600905_006_01.h5\n",
      "  ‚úÖ gt1l: 240 pts ‚Üí ATL06_0960_gt1l_20201126.shp\n",
      "  ‚úÖ gt1r: 233 pts ‚Üí ATL06_0960_gt1r_20201126.shp\n",
      "  ‚úÖ gt2l: 50 pts ‚Üí ATL06_0960_gt2l_20201126.shp\n",
      "  ‚úÖ gt2r: 50 pts ‚Üí ATL06_0960_gt2r_20201126.shp\n",
      "  ‚úÖ gt3l: 54 pts ‚Üí ATL06_0960_gt3l_20201126.shp\n",
      "  ‚úÖ gt3r: 53 pts ‚Üí ATL06_0960_gt3r_20201126.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20210224204813_09601005_006_01.h5\n",
      "  ‚úÖ gt1l: 241 pts ‚Üí ATL06_0960_gt1l_20210224.shp\n",
      "  ‚úÖ gt1r: 233 pts ‚Üí ATL06_0960_gt1r_20210224.shp\n",
      "  ‚úÖ gt2l: 50 pts ‚Üí ATL06_0960_gt2l_20210224.shp\n",
      "  ‚úÖ gt2r: 50 pts ‚Üí ATL06_0960_gt2r_20210224.shp\n",
      "  ‚úÖ gt3l: 54 pts ‚Üí ATL06_0960_gt3l_20210224.shp\n",
      "  ‚úÖ gt3r: 52 pts ‚Üí ATL06_0960_gt3r_20210224.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20210526162804_09601105_006_01.h5\n",
      "  ‚ö†Ô∏è  gt1l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt1r: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt2l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt2r: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt3l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt3r: no points within 500 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20210825120758_09601205_006_01.h5\n",
      "  ‚úÖ gt1l: 112 pts ‚Üí ATL06_0960_gt1l_20210825.shp\n",
      "  ‚úÖ gt1r: 129 pts ‚Üí ATL06_0960_gt1r_20210825.shp\n",
      "  ‚úÖ gt2l: 6 pts ‚Üí ATL06_0960_gt2l_20210825.shp\n",
      "  ‚úÖ gt2r: 13 pts ‚Üí ATL06_0960_gt2r_20210825.shp\n",
      "  ‚ö†Ô∏è  gt3l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt3r: no points within 500 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20211124074757_09601305_006_01.h5\n",
      "  ‚úÖ gt1l: 240 pts ‚Üí ATL06_0960_gt1l_20211124.shp\n",
      "  ‚úÖ gt1r: 233 pts ‚Üí ATL06_0960_gt1r_20211124.shp\n",
      "  ‚úÖ gt2l: 50 pts ‚Üí ATL06_0960_gt2l_20211124.shp\n",
      "  ‚úÖ gt2r: 50 pts ‚Üí ATL06_0960_gt2r_20211124.shp\n",
      "  ‚úÖ gt3l: 54 pts ‚Üí ATL06_0960_gt3l_20211124.shp\n",
      "  ‚úÖ gt3r: 52 pts ‚Üí ATL06_0960_gt3r_20211124.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20220223032746_09601405_006_01.h5\n",
      "  ‚ö†Ô∏è  gt1l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt1r: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt2l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt2r: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt3l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt3r: no points within 500 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20220524230736_09601505_006_01.h5\n",
      "  ‚úÖ gt1l: 22 pts ‚Üí ATL06_0960_gt1l_20220524.shp\n",
      "  ‚úÖ gt1r: 6 pts ‚Üí ATL06_0960_gt1r_20220524.shp\n",
      "  ‚úÖ gt2l: 23 pts ‚Üí ATL06_0960_gt2l_20220524.shp\n",
      "  ‚úÖ gt2r: 2 pts ‚Üí ATL06_0960_gt2r_20220524.shp\n",
      "  ‚úÖ gt3l: 43 pts ‚Üí ATL06_0960_gt3l_20220524.shp\n",
      "  ‚úÖ gt3r: 16 pts ‚Üí ATL06_0960_gt3r_20220524.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20220823184739_09601605_006_01.h5\n",
      "  ‚ö†Ô∏è  gt1l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt1r: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt2l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt2r: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt3l: no points within 500 m buffer.\n",
      "  ‚ö†Ô∏è  gt3r: no points within 500 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20221122142717_09601705_006_02.h5\n",
      "  ‚úÖ gt1l: 175 pts ‚Üí ATL06_0960_gt1l_20221122.shp\n",
      "  ‚úÖ gt1r: 234 pts ‚Üí ATL06_0960_gt1r_20221122.shp\n",
      "  ‚úÖ gt2l: 16 pts ‚Üí ATL06_0960_gt2l_20221122.shp\n",
      "  ‚úÖ gt2r: 46 pts ‚Üí ATL06_0960_gt2r_20221122.shp\n",
      "  ‚úÖ gt3l: 40 pts ‚Üí ATL06_0960_gt3l_20221122.shp\n",
      "  ‚úÖ gt3r: 52 pts ‚Üí ATL06_0960_gt3r_20221122.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20230221100711_09601805_006_02.h5\n",
      "  ‚úÖ gt1l: 235 pts ‚Üí ATL06_0960_gt1l_20230221.shp\n",
      "  ‚úÖ gt1r: 235 pts ‚Üí ATL06_0960_gt1r_20230221.shp\n",
      "  ‚úÖ gt2l: 50 pts ‚Üí ATL06_0960_gt2l_20230221.shp\n",
      "  ‚úÖ gt2r: 50 pts ‚Üí ATL06_0960_gt2r_20230221.shp\n",
      "  ‚úÖ gt3l: 54 pts ‚Üí ATL06_0960_gt3l_20230221.shp\n",
      "  ‚úÖ gt3r: 53 pts ‚Üí ATL06_0960_gt3r_20230221.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20230523054646_09601905_006_03.h5\n",
      "  ‚úÖ gt1l: 239 pts ‚Üí ATL06_0960_gt1l_20230523.shp\n",
      "  ‚úÖ gt1r: 235 pts ‚Üí ATL06_0960_gt1r_20230523.shp\n",
      "  ‚úÖ gt2l: 50 pts ‚Üí ATL06_0960_gt2l_20230523.shp\n",
      "  ‚úÖ gt2r: 50 pts ‚Üí ATL06_0960_gt2r_20230523.shp\n",
      "  ‚úÖ gt3l: 54 pts ‚Üí ATL06_0960_gt3l_20230523.shp\n",
      "  ‚úÖ gt3r: 53 pts ‚Üí ATL06_0960_gt3r_20230523.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20230822012553_09602005_006_02.h5\n",
      "  ‚úÖ gt1l: 235 pts ‚Üí ATL06_0960_gt1l_20230822.shp\n",
      "  ‚úÖ gt1r: 226 pts ‚Üí ATL06_0960_gt1r_20230822.shp\n",
      "  ‚úÖ gt2l: 51 pts ‚Üí ATL06_0960_gt2l_20230822.shp\n",
      "  ‚úÖ gt2r: 46 pts ‚Üí ATL06_0960_gt2r_20230822.shp\n",
      "  ‚úÖ gt3l: 7 pts ‚Üí ATL06_0960_gt3l_20230822.shp\n",
      "  ‚ö†Ô∏è  gt3r: no points within 500 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20231120210543_09602105_006_02.h5\n",
      "  ‚úÖ gt1l: 114 pts ‚Üí ATL06_0960_gt1l_20231120.shp\n",
      "  ‚úÖ gt1r: 170 pts ‚Üí ATL06_0960_gt1r_20231120.shp\n",
      "  ‚úÖ gt2l: 25 pts ‚Üí ATL06_0960_gt2l_20231120.shp\n",
      "  ‚úÖ gt2r: 50 pts ‚Üí ATL06_0960_gt2r_20231120.shp\n",
      "  ‚úÖ gt3l: 17 pts ‚Üí ATL06_0960_gt3l_20231120.shp\n",
      "  ‚úÖ gt3r: 53 pts ‚Üí ATL06_0960_gt3r_20231120.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20240219164503_09602205_006_01.h5\n",
      "  ‚úÖ gt1l: 237 pts ‚Üí ATL06_0960_gt1l_20240219.shp\n",
      "  ‚úÖ gt1r: 235 pts ‚Üí ATL06_0960_gt1r_20240219.shp\n",
      "  ‚úÖ gt2l: 50 pts ‚Üí ATL06_0960_gt2l_20240219.shp\n",
      "  ‚úÖ gt2r: 50 pts ‚Üí ATL06_0960_gt2r_20240219.shp\n",
      "  ‚úÖ gt3l: 54 pts ‚Üí ATL06_0960_gt3l_20240219.shp\n",
      "  ‚úÖ gt3r: 53 pts ‚Üí ATL06_0960_gt3r_20240219.shp\n",
      "\n",
      "üìÇ Processing: ATL06_20241118034407_09602505_006_01.h5\n",
      "  ‚úÖ gt1l: 348 pts ‚Üí ATL06_0960_gt1l_20241118.shp\n",
      "  ‚úÖ gt1r: 363 pts ‚Üí ATL06_0960_gt1r_20241118.shp\n",
      "  ‚úÖ gt2l: 50 pts ‚Üí ATL06_0960_gt2l_20241118.shp\n",
      "  ‚úÖ gt2r: 38 pts ‚Üí ATL06_0960_gt2r_20241118.shp\n",
      "  ‚úÖ gt3l: 13 pts ‚Üí ATL06_0960_gt3l_20241118.shp\n",
      "  ‚ö†Ô∏è  gt3r: no points within 500 m buffer.\n",
      "\n",
      "üìÇ Processing: ATL06_20250216232346_09602605_006_01.h5\n",
      "  ‚úÖ gt1l: 230 pts ‚Üí ATL06_0960_gt1l_20250216.shp\n",
      "  ‚úÖ gt1r: 190 pts ‚Üí ATL06_0960_gt1r_20250216.shp\n",
      "  ‚úÖ gt2l: 50 pts ‚Üí ATL06_0960_gt2l_20250216.shp\n",
      "  ‚úÖ gt2r: 50 pts ‚Üí ATL06_0960_gt2r_20250216.shp\n",
      "  ‚úÖ gt3l: 53 pts ‚Üí ATL06_0960_gt3l_20250216.shp\n",
      "  ‚úÖ gt3r: 52 pts ‚Üí ATL06_0960_gt3r_20250216.shp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === USER INPUTS ===\n",
    "input_folder = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\andre\\0960')  # Folder with ICESat-2 .h5 files\n",
    "coastline_path = pl.Path(r'C:\\coding\\arctic\\Gis\\datasets\\MyOwn\\AOI_shoreline.shp')  # Polyline shapefile\n",
    "output_folder = input_folder / \"filtered2\"  # Output folder\n",
    "beam_groups = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "buffer_dist = 500  # meters\n",
    "\n",
    "WRITE_GPKG = False         # set True if you want GeoPackage too\n",
    "WRITE_GEOPARQUET = False    # modern + fast format\n",
    "\n",
    "# === HELPERS ===\n",
    "GEOD = Geod(ellps=\"WGS84\")\n",
    "\n",
    "def cumdist_geodesic(lon, lat):\n",
    "    \"\"\"\n",
    "    Cumulative geodesic distance (meters) along the given lon/lat sequence.\n",
    "    Assumes the sequence is already ordered the way you want (e.g., N->S).\n",
    "    \"\"\"\n",
    "    lon = np.asarray(lon, dtype=float)\n",
    "    lat = np.asarray(lat, dtype=float)\n",
    "    n = len(lon)\n",
    "    if n == 0:\n",
    "        return np.array([], dtype=float)\n",
    "    if n == 1:\n",
    "        return np.array([0.0], dtype=float)\n",
    "    # pairwise distances\n",
    "    _, _, d = GEOD.inv(lon[:-1], lat[:-1], lon[1:], lat[1:])\n",
    "    return np.concatenate(([0.0], np.cumsum(d)))\n",
    "\n",
    "def safe_first(arr):\n",
    "    try:\n",
    "        return arr[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def parse_date_track_from_name(h5_path):\n",
    "    \"\"\"\n",
    "    Parse YYYYMMDD and 4-digit track_id from ATL06 filenames like:\n",
    "    ATL06_20190105212430_01290203_006_01.h5\n",
    "                ^^^^^^^^  ^^^^\n",
    "    Returns (date_str, track_id) or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    stem = Path(h5_path).stem\n",
    "    parts = stem.split('_')\n",
    "\n",
    "    date = None\n",
    "    track_id = None\n",
    "\n",
    "    # Primary: strict per your rule\n",
    "    if len(parts) >= 3:\n",
    "        # parts[1] = 'YYYYMMDDHHMMSS' ‚Üí take first 8\n",
    "        if parts[1].isdigit() and len(parts[1]) >= 8:\n",
    "            date = parts[1][:8]\n",
    "        # parts[2] = '01290203' ‚Üí take first 4 as track\n",
    "        if parts[2].isdigit() and len(parts[2]) >= 4:\n",
    "            track_id = parts[2][:4]\n",
    "\n",
    "    # Fallback: regex (handles minor naming variations)\n",
    "    if date is None or track_id is None:\n",
    "        m = re.search(r'_(\\d{8})(?:\\d{6})?_([0-9]{4})', stem)\n",
    "        if m:\n",
    "            date = date or m.group(1)\n",
    "            track_id = track_id or m.group(2)\n",
    "\n",
    "    return date, track_id\n",
    "\n",
    "# === PREP ===\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Read coastline and build a single buffer polygon in meters CRS (EPSG:3413)\n",
    "coastline = gpd.read_file(coastline_path).to_crs(\"EPSG:3413\")\n",
    "# If the shoreline layer has multiple parts, .buffer() then unary_union gives one geometry\n",
    "coast_buffer_geom = coastline.buffer(buffer_dist).union_all()  # shapely (Multi)Polygon\n",
    "\n",
    "# === PROCESS EACH .H5 FILE ===\n",
    "for h5_file in sorted(input_folder.glob(\"*.h5\")):\n",
    "    print(f\"\\nüìÇ Processing: {h5_file.name}\")\n",
    "    try:\n",
    "        for beam in beam_groups:\n",
    "            try:\n",
    "                group = f'/{beam}/land_ice_segments'\n",
    "                # Open as context to ensure clean close\n",
    "                with xr.open_dataset(h5_file, group=group, engine='h5netcdf') as ds:\n",
    "                    # Required vars (drop NaNs right away)\n",
    "                    if not all(v in ds.variables for v in ['latitude', 'longitude', 'h_li']):\n",
    "                        print(f\"  ‚ö†Ô∏è  {beam}: missing required vars; skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    lat = ds['latitude'].values\n",
    "                    lon = ds['longitude'].values\n",
    "                    h_li = ds['h_li'].values\n",
    "\n",
    "                    # Quality filter (optional; comment out if you don't want it)\n",
    "                    # Keep 'good' segments only if available\n",
    "                    if 'atl06_quality_summary' in ds.variables:\n",
    "                        q = ds['atl06_quality_summary'].values\n",
    "                        good = (q == 0)\n",
    "                        lat, lon, h_li = lat[good], lon[good], h_li[good]\n",
    "\n",
    "                    # Drop NaNs\n",
    "                    m = np.isfinite(lat) & np.isfinite(lon) & np.isfinite(h_li)\n",
    "                    lat, lon, h_li = lat[m], lon[m], h_li[m]\n",
    "                    if lat.size == 0:\n",
    "                        print(f\"  ‚ö†Ô∏è  {beam}: no valid points after QC/NaN filter.\")\n",
    "                        continue\n",
    "\n",
    "                    # Metadata: date & track (prefer variables, fallback to filename)\n",
    "                    date_str, track_id_name = parse_date_track_from_name(h5_file)\n",
    "\n",
    "                    # RGT and cycle (if present)\n",
    "                    rgt = None\n",
    "                    cycle = None\n",
    "                    if 'rgt' in ds.variables:\n",
    "                        try:\n",
    "                            rgt = int(np.nanmedian(ds['rgt'].values))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    if 'cycle_number' in ds.variables:\n",
    "                        try:\n",
    "                            cycle = int(np.nanmedian(ds['cycle_number'].values))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    # Fallbacks\n",
    "                    if track_id_name is None and rgt is not None:\n",
    "                        track_id_name = f\"{rgt:04d}\"\n",
    "\n",
    "                    # Build GeoDataFrame in lon/lat, then project to meters CRS for clipping\n",
    "                    gdf = gpd.GeoDataFrame(\n",
    "                        {\n",
    "                            'latitude': lat,\n",
    "                            'longitude': lon,\n",
    "                            'h_li': h_li,\n",
    "                            'track_id': track_id_name,\n",
    "                            'gt': beam,\n",
    "                            'date': date_str,\n",
    "                            'rgt': rgt,\n",
    "                            'cycle': cycle,\n",
    "                        },\n",
    "                        geometry=gpd.points_from_xy(lon, lat),\n",
    "                        crs=\"EPSG:4326\",\n",
    "                    ).to_crs(\"EPSG:3413\")\n",
    "\n",
    "                    # --- EARLY CLIP TO BUFFER (includes boundary) ---\n",
    "                    # For points, 'intersects' behaves like \"inside or on boundary\"\n",
    "                    in_buf = gdf.geometry.intersects(coast_buffer_geom)\n",
    "                    selected = gdf.loc[in_buf].copy()\n",
    "\n",
    "                    if selected.empty:\n",
    "                        print(f\"  ‚ö†Ô∏è  {beam}: no points within {buffer_dist} m buffer.\")\n",
    "                        continue\n",
    "\n",
    "                    # Restore lon/lat columns after projection (keep both CRSes if you like)\n",
    "                    selected_ll = selected.to_crs(\"EPSG:4326\")\n",
    "                    selected['latitude'] = selected_ll.geometry.y.values\n",
    "                    selected['longitude'] = selected_ll.geometry.x.values\n",
    "\n",
    "                    # --- ORDER & DISTANCE: start at northernmost point ---\n",
    "                    selected.sort_values('latitude', ascending=False, inplace=True)\n",
    "                    # Use geodesic cumulative distance along lon/lat (meters)\n",
    "                    dists = cumdist_geodesic(selected['longitude'].values,\n",
    "                                             selected['latitude'].values)\n",
    "                    selected['distance_m'] = dists\n",
    "\n",
    "                    # --- WRITE OUTPUTS ---\n",
    "                    out_stem = f\"ATL06_{selected['track_id'].iloc[0] or 'unk'}_{beam}_{date_str or 'nodate'}\"\n",
    "                    shp_path = output_folder / f\"{out_stem}.shp\"\n",
    "                    selected.to_file(shp_path)\n",
    "                    print(f\"  ‚úÖ {beam}: {len(selected)} pts ‚Üí {shp_path.name}\")\n",
    "\n",
    "                    if WRITE_GPKG:\n",
    "                        gpkg_path = output_folder / f\"{out_stem}.gpkg\"\n",
    "                        selected.to_file(gpkg_path, driver=\"GPKG\")\n",
    "                    if WRITE_GEOPARQUET:\n",
    "                        parquet_path = output_folder / f\"{out_stem}.parquet\"\n",
    "                        selected.to_parquet(parquet_path, index=False)\n",
    "\n",
    "            except Exception as beam_error:\n",
    "                print(f\"  ‚ö†Ô∏è  Skipping {beam} in {h5_file.name}: {beam_error}\")\n",
    "\n",
    "    except Exception as file_error:\n",
    "        print(f\"‚ùå Failed to process {h5_file.name}: {file_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Processing: ATL06_20190604030110_10210305_006_02.h5\n",
      "‚úÖ gt1l: 44 points saved.\n",
      "‚úÖ gt1r: 44 points saved.\n",
      "‚úÖ gt2l: 40 points saved.\n",
      "‚úÖ gt2r: 40 points saved.\n",
      "‚úÖ gt3l: 39 points saved.\n",
      "‚úÖ gt3r: 41 points saved.\n",
      "üìÇ Processing: ATL06_20190902224059_10210405_006_02.h5\n",
      "‚úÖ gt1l: 51 points saved.\n",
      "‚úÖ gt1r: 51 points saved.\n",
      "‚úÖ gt2l: 50 points saved.\n",
      "‚úÖ gt2r: 50 points saved.\n",
      "‚úÖ gt3l: 62 points saved.\n",
      "‚úÖ gt3r: 62 points saved.\n",
      "üìÇ Processing: ATL06_20191202182049_10210505_006_01.h5\n",
      "‚ö†Ô∏è gt1l: No points within buffer.\n",
      "‚ö†Ô∏è gt1r: No points within buffer.\n",
      "‚ö†Ô∏è gt2l: No points within buffer.\n",
      "‚ö†Ô∏è gt2r: No points within buffer.\n",
      "‚ö†Ô∏è gt3l: No points within buffer.\n",
      "‚ö†Ô∏è gt3r: No points within buffer.\n",
      "üìÇ Processing: ATL06_20200302140033_10210605_006_01.h5\n",
      "‚úÖ gt1l: 51 points saved.\n",
      "‚úÖ gt1r: 51 points saved.\n",
      "‚úÖ gt2l: 50 points saved.\n",
      "‚úÖ gt2r: 50 points saved.\n",
      "‚úÖ gt3l: 62 points saved.\n",
      "‚úÖ gt3r: 62 points saved.\n",
      "üìÇ Processing: ATL06_20200601094024_10210705_006_01.h5\n",
      "‚úÖ gt1l: 51 points saved.\n",
      "‚úÖ gt1r: 51 points saved.\n",
      "‚úÖ gt2l: 50 points saved.\n",
      "‚úÖ gt2r: 50 points saved.\n",
      "‚úÖ gt3l: 63 points saved.\n",
      "‚úÖ gt3r: 62 points saved.\n",
      "üìÇ Processing: ATL06_20201130005959_10210905_006_01.h5\n",
      "‚úÖ gt1l: 51 points saved.\n",
      "‚úÖ gt1r: 51 points saved.\n",
      "‚úÖ gt2l: 50 points saved.\n",
      "‚úÖ gt2r: 50 points saved.\n",
      "‚úÖ gt3l: 63 points saved.\n",
      "‚úÖ gt3r: 63 points saved.\n",
      "üìÇ Processing: ATL06_20210228203951_10211005_006_01.h5\n",
      "‚úÖ gt1l: 51 points saved.\n",
      "‚úÖ gt1r: 51 points saved.\n",
      "‚úÖ gt2l: 50 points saved.\n",
      "‚úÖ gt2r: 50 points saved.\n",
      "‚úÖ gt3l: 62 points saved.\n",
      "‚úÖ gt3r: 62 points saved.\n",
      "üìÇ Processing: ATL06_20210530161944_10211105_006_01.h5\n",
      "‚úÖ gt1l: 14 points saved.\n",
      "‚úÖ gt1r: 14 points saved.\n",
      "‚úÖ gt2l: 8 points saved.\n",
      "‚úÖ gt2r: 8 points saved.\n",
      "‚ö†Ô∏è gt3l: No points within buffer.\n",
      "‚ö†Ô∏è gt3r: No points within buffer.\n",
      "üìÇ Processing: ATL06_20211128073936_10211305_006_01.h5\n",
      "‚úÖ gt1l: 51 points saved.\n",
      "‚úÖ gt1r: 51 points saved.\n",
      "‚úÖ gt2l: 50 points saved.\n",
      "‚úÖ gt2r: 50 points saved.\n",
      "‚úÖ gt3l: 62 points saved.\n",
      "‚úÖ gt3r: 62 points saved.\n",
      "üìÇ Processing: ATL06_20220227031927_10211405_006_01.h5\n",
      "‚ö†Ô∏è gt1l: No points within buffer.\n",
      "‚ö†Ô∏è gt1r: No points within buffer.\n",
      "‚ö†Ô∏è gt2l: No points within buffer.\n",
      "‚ö†Ô∏è gt2r: No points within buffer.\n",
      "‚ö†Ô∏è gt3l: No points within buffer.\n",
      "‚ö†Ô∏è gt3r: No points within buffer.\n",
      "üìÇ Processing: ATL06_20220528225920_10211505_006_01.h5\n",
      "‚ö†Ô∏è gt1l: No points within buffer.\n",
      "‚ö†Ô∏è gt1r: No points within buffer.\n",
      "‚ö†Ô∏è gt2l: No points within buffer.\n",
      "‚ö†Ô∏è gt2r: No points within buffer.\n",
      "‚ö†Ô∏è gt3l: No points within buffer.\n",
      "‚ö†Ô∏è gt3r: No points within buffer.\n",
      "üìÇ Processing: ATL06_20220827183921_10211605_006_01.h5\n",
      "‚ö†Ô∏è gt1l: No points within buffer.\n",
      "‚ö†Ô∏è gt1r: No points within buffer.\n",
      "‚ö†Ô∏è gt2l: No points within buffer.\n",
      "‚ö†Ô∏è gt2r: No points within buffer.\n",
      "‚úÖ gt3l: 2 points saved.\n",
      "‚úÖ gt3r: 2 points saved.\n",
      "üìÇ Processing: ATL06_20230225095903_10211805_006_02.h5\n",
      "‚úÖ gt1l: 43 points saved.\n",
      "‚úÖ gt1r: 43 points saved.\n",
      "‚úÖ gt2l: 15 points saved.\n",
      "‚úÖ gt2r: 15 points saved.\n",
      "‚úÖ gt3l: 23 points saved.\n",
      "‚úÖ gt3r: 22 points saved.\n",
      "üìÇ Processing: ATL06_20230527053822_10211905_006_03.h5\n",
      "‚úÖ gt1l: 51 points saved.\n",
      "‚úÖ gt1r: 51 points saved.\n",
      "‚úÖ gt2l: 50 points saved.\n",
      "‚úÖ gt2r: 50 points saved.\n",
      "‚úÖ gt3l: 63 points saved.\n",
      "‚úÖ gt3r: 62 points saved.\n",
      "üìÇ Processing: ATL06_20230826011739_10212005_006_02.h5\n",
      "‚ö†Ô∏è gt1l: No points within buffer.\n",
      "‚ö†Ô∏è gt1r: No points within buffer.\n",
      "‚úÖ gt2l: 1 points saved.\n",
      "‚úÖ gt2r: 1 points saved.\n",
      "‚úÖ gt3l: 2 points saved.\n",
      "‚úÖ gt3r: 2 points saved.\n",
      "üìÇ Processing: ATL06_20231124205713_10212105_006_02.h5\n",
      "‚ö†Ô∏è gt1l: No points within buffer.\n",
      "‚ö†Ô∏è gt1r: No points within buffer.\n",
      "‚úÖ gt2l: 1 points saved.\n",
      "‚úÖ gt2r: 1 points saved.\n",
      "‚úÖ gt3l: 14 points saved.\n",
      "‚úÖ gt3r: 18 points saved.\n",
      "üìÇ Processing: ATL06_20240223163640_10212205_006_01.h5\n",
      "‚úÖ gt1l: 51 points saved.\n",
      "‚úÖ gt1r: 51 points saved.\n",
      "‚úÖ gt2l: 50 points saved.\n",
      "‚úÖ gt2r: 50 points saved.\n",
      "‚úÖ gt3l: 63 points saved.\n",
      "‚úÖ gt3r: 62 points saved.\n",
      "üìÇ Processing: ATL06_20240823075558_10212405_006_01.h5\n",
      "‚ö†Ô∏è gt1l: No points within buffer.\n",
      "‚ö†Ô∏è gt1r: No points within buffer.\n",
      "‚ö†Ô∏è gt2l: No points within buffer.\n",
      "‚ö†Ô∏è gt2r: No points within buffer.\n",
      "‚úÖ gt3l: 33 points saved.\n",
      "‚úÖ gt3r: 32 points saved.\n",
      "üìÇ Processing: ATL06_20241122033533_10212505_006_01.h5\n",
      "‚ö†Ô∏è gt1l: No points within buffer.\n",
      "‚ö†Ô∏è gt1r: No points within buffer.\n",
      "‚ö†Ô∏è gt2l: No points within buffer.\n",
      "‚ö†Ô∏è gt2r: No points within buffer.\n",
      "‚ö†Ô∏è gt3l: No points within buffer.\n",
      "‚ö†Ô∏è gt3r: No points within buffer.\n",
      "üìÇ Processing: ATL06_20250220231510_10212605_006_01.h5\n",
      "‚úÖ gt1l: 4 points saved.\n",
      "‚úÖ gt1r: 4 points saved.\n",
      "‚úÖ gt2l: 38 points saved.\n",
      "‚úÖ gt2r: 38 points saved.\n",
      "‚úÖ gt3l: 6 points saved.\n",
      "‚úÖ gt3r: 5 points saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === USER INPUTS ===\n",
    "input_folder = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\andre\\1021')  # Folder with ICESat-2 .h5 files\n",
    "coastline_path = pl.Path(r'C:\\coding\\arctic\\Gis\\datasets\\MyOwn\\AOI_shoreline.shp')  # Polyline shapefile\n",
    "output_folder = input_folder / \"filtered2\"  # Output folder\n",
    "beam_groups = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "buffer_dist = 500  # meters\n",
    "\n",
    "# === PREP ===\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "coastline = gpd.read_file(coastline_path).to_crs(\"EPSG:3413\")\n",
    "coast_buffer_union = gpd.GeoSeries(coastline.buffer(buffer_dist).union_all(), crs=coastline.crs)\n",
    "\n",
    "# === PROCESS EACH .H5 FILE ===\n",
    "for h5_file in input_folder.glob(\"*.h5\"):\n",
    "    print(f\"üìÇ Processing: {h5_file.name}\")\n",
    "    try:\n",
    "        for beam in beam_groups:\n",
    "            try:\n",
    "                ds = xr.open_dataset(h5_file, group=f'/{beam}/land_ice_segments', engine='h5netcdf')\n",
    "\n",
    "                # Extract lat/lon/h_li\n",
    "                lat = ds['latitude'].values\n",
    "                lon = ds['longitude'].values\n",
    "                h_li = ds['h_li'].values\n",
    "\n",
    "                # Extract date and track_id from filename\n",
    "                parts = h5_file.stem.split('_')\n",
    "                datetime_str = parts[1]        # '20190105212430'\n",
    "                track_info = parts[2]          # '01290203'\n",
    "                date = datetime_str[:8]        # '20190105'\n",
    "                track_id = track_info[:4]      # '0129'\n",
    "\n",
    "                # Calculate the distance between each point\n",
    "                distance = np.zeros(lat.shape)\n",
    "                for i in range(1, len(lat)):\n",
    "                    distance[i] = haversine_distance(lat[0], lon[0], lat[i], lon[i])\n",
    "                distance = distance * 1000  # convert to meters\n",
    "\n",
    "                # Create GeoDataFrame\n",
    "                df = pd.DataFrame({\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'h_li': h_li,\n",
    "                    'distance': distance,\n",
    "                    'track_id': track_id,\n",
    "                    'gt': beam,\n",
    "                    'date': date\n",
    "                })\n",
    "                gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=\"EPSG:4326\")\n",
    "                gdf = gdf.to_crs(\"EPSG:3413\")\n",
    "\n",
    "                # Clip to buffer\n",
    "                selected = gdf[gdf.geometry.within(coast_buffer_union[0])]\n",
    "\n",
    "                if not selected.empty:\n",
    "                    out_name = f\"ATL06_{track_id}_{beam}_{date}.shp\"\n",
    "                    selected.to_file(output_folder / out_name)\n",
    "                    print(f\"‚úÖ {beam}: {len(selected)} points saved.\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è {beam}: No points within buffer.\")\n",
    "\n",
    "            except Exception as beam_error:\n",
    "                print(f\"‚ö†Ô∏è Skipping beam {beam} in {h5_file.name}: {beam_error}\")\n",
    "\n",
    "    except Exception as file_error:\n",
    "        print(f\"‚ùå Failed to process {h5_file.name}: {file_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\andre')\n",
    "files = list(path.glob('*.h5'))\n",
    "names = ['gt1r', 'gt1l', 'gt2r', 'gt2l', 'gt3r', 'gt3l']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of subfolders inside a folder\n",
    "# count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    # Extract the track name (e.g., '0137')\n",
    "    track_name = f.name.split('_')[2][:4]\n",
    "\n",
    "    # Create the new subfolder in the same directory as the file\n",
    "    subfolder = f.parent / track_name\n",
    "    subfolder.mkdir(exist_ok=True)\n",
    "\n",
    "    # Destination path\n",
    "    destination = subfolder / f.name\n",
    "\n",
    "    # Copy the file\n",
    "    shutil.move(f, destination)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform to SHP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\nasa\\0876')\n",
    "files = list(path.glob('*.h5'))\n",
    "names = ['gt1r', 'gt1l', 'gt2r', 'gt2l', 'gt3r', 'gt3l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all files and names to create shapefiles for each group\n",
    "\n",
    "for file in files:\n",
    "    for name in names:\n",
    "        try:\n",
    "            ds = xr.open_dataset(file, group=f'/{name}/land_ice_segments', engine='h5netcdf')\n",
    "        except OSError:\n",
    "            print(f\"Group {name} in file {file} is empty or does not exist. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        lat = ds['latitude']\n",
    "        lon = ds['longitude']\n",
    "        h_li = ds['h_li']\n",
    "        \n",
    "        # Calculate the distance between each point\n",
    "        distance = np.zeros(lat.shape)\n",
    "        for i in range(1, len(lat)):\n",
    "            distance[i] = haversine_distance(lat[0], lon[0], lat[i], lon[i])\n",
    "        distance = distance * 1000  # convert to meters\n",
    "        \n",
    "        date = file.stem.split('_')[2][:8]\n",
    "        track_id = file.stem.split('_')[3][:4]\n",
    "        \n",
    "        data = {\n",
    "            'latitude': lat,\n",
    "            'longitude': lon,\n",
    "            'h_li': h_li,\n",
    "            'distance': distance,\n",
    "            'date': date,\n",
    "            'gt': name,\n",
    "            'track_id': track_id\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "        gdf.set_crs(epsg=4326, inplace=True)\n",
    "        \n",
    "        # salve shapefile in the same directory as the file\n",
    "        savepath = file.parent / 'shapefiles'\n",
    "        savepath.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "      \n",
    "        shapefile_name = f\"ATL06_{track_id}_{name}_{date}.shp\"\n",
    "        atl06 = savepath / shapefile_name\n",
    "        gdf.to_file(atl06)\n",
    "        \n",
    "        # print(f\"Shapefile saved to {atl06}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\nasa\\0137')\n",
    "files = list(path.glob('*.h5'))\n",
    "names = ['gt1r', 'gt1l', 'gt2r', 'gt2l', 'gt3r', 'gt3l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all files and names to create shapefiles for each group\n",
    "\n",
    "for file in files:\n",
    "    for name in names:\n",
    "        try:\n",
    "            ds = xr.open_dataset(file, group=f'/{name}/land_ice_segments', engine='h5netcdf')\n",
    "        except OSError:\n",
    "            print(f\"Group {name} in file {file} is empty or does not exist. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        lat = ds['latitude']\n",
    "        lon = ds['longitude']\n",
    "        h_li = ds['h_li']\n",
    "        \n",
    "        # Calculate the distance between each point\n",
    "        distance = np.zeros(lat.shape)\n",
    "        for i in range(1, len(lat)):\n",
    "            distance[i] = haversine_distance(lat[0], lon[0], lat[i], lon[i])\n",
    "        distance = distance * 1000  # convert to meters\n",
    "        \n",
    "        date = file.stem.split('_')[2][:8]\n",
    "        track_id = file.stem.split('_')[3][:4]\n",
    "        \n",
    "        data = {\n",
    "            'latitude': lat,\n",
    "            'longitude': lon,\n",
    "            'h_li': h_li,\n",
    "            'distance': distance,\n",
    "            'date': date,\n",
    "            'gt': name,\n",
    "            'track_id': track_id\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "        gdf.set_crs(epsg=4326, inplace=True)\n",
    "        \n",
    "        savepath = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\nasa\\0137')\n",
    "        savepath.mkdir(parents=True, exist_ok=True)\n",
    "        shapefile_name = f\"ATL06_{track_id}_{name}_{date}.shp\"\n",
    "        atl06 = savepath / shapefile_name\n",
    "        gdf.to_file(atl06)\n",
    "        \n",
    "        # print(f\"Shapefile saved to {atl06}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\2025\\0769')\n",
    "files = list(path.glob('*.h5'))\n",
    "names = ['gt1r', 'gt1l', 'gt2r', 'gt2l', 'gt3r', 'gt3l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all files and names to create shapefiles for each group\n",
    "\n",
    "for file in files:\n",
    "    for name in names:\n",
    "        try:\n",
    "            ds = xr.open_dataset(file, group=f'/{name}/land_ice_segments', engine='h5netcdf')\n",
    "        except OSError:\n",
    "            print(f\"Group {name} in file {file} is empty or does not exist. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        lat = ds['latitude']\n",
    "        lon = ds['longitude']\n",
    "        h_li = ds['h_li']\n",
    "        \n",
    "        # Calculate the distance between each point\n",
    "        distance = np.zeros(lat.shape)\n",
    "        for i in range(1, len(lat)):\n",
    "            distance[i] = haversine_distance(lat[0], lon[0], lat[i], lon[i])\n",
    "        distance = distance * 1000  # convert to meters\n",
    "        \n",
    "        date = file.stem.split('_')[2][:8]\n",
    "        track_id = file.stem.split('_')[3][:4]\n",
    "        \n",
    "        data = {\n",
    "            'latitude': lat,\n",
    "            'longitude': lon,\n",
    "            'h_li': h_li,\n",
    "            'distance': distance,\n",
    "            'date': date,\n",
    "            'gt': name,\n",
    "            'track_id': track_id\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "        gdf.set_crs(epsg=4326, inplace=True)\n",
    "        \n",
    "        savepath = pl.Path(r'C:\\coding\\arctic\\paper1\\shp\\Colorado\\Extras')\n",
    "        savepath.mkdir(parents=True, exist_ok=True)\n",
    "        shapefile_name = f\"ATL06_{track_id}_{name}_{date}.shp\"\n",
    "        atl06 = savepath / shapefile_name\n",
    "        gdf.to_file(atl06)\n",
    "        \n",
    "        # print(f\"Shapefile saved to {atl06}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\2025\\0525')\n",
    "files = list(path.glob('*.h5'))\n",
    "names = ['gt1r', 'gt1l', 'gt2r', 'gt2l', 'gt3r', 'gt3l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all files and names to create shapefiles for each group\n",
    "\n",
    "for file in files:\n",
    "    for name in names:\n",
    "        try:\n",
    "            ds = xr.open_dataset(file, group=f'/{name}/land_ice_segments', engine='h5netcdf')\n",
    "        except OSError:\n",
    "            print(f\"Group {name} in file {file} is empty or does not exist. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        lat = ds['latitude']\n",
    "        lon = ds['longitude']\n",
    "        h_li = ds['h_li']\n",
    "        \n",
    "        # Calculate the distance between each point\n",
    "        distance = np.zeros(lat.shape)\n",
    "        for i in range(1, len(lat)):\n",
    "            distance[i] = haversine_distance(lat[0], lon[0], lat[i], lon[i])\n",
    "        distance = distance * 1000  # convert to meters\n",
    "        \n",
    "        date = file.stem.split('_')[2][:8]\n",
    "        track_id = file.stem.split('_')[3][:4]\n",
    "        \n",
    "        data = {\n",
    "            'latitude': lat,\n",
    "            'longitude': lon,\n",
    "            'h_li': h_li,\n",
    "            'distance': distance,\n",
    "            'date': date,\n",
    "            'gt': name,\n",
    "            'track_id': track_id\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "        gdf.set_crs(epsg=4326, inplace=True)\n",
    "        \n",
    "        savepath = pl.Path(r'C:\\coding\\arctic\\paper1\\shp\\Colorado\\Extras')\n",
    "        savepath.mkdir(parents=True, exist_ok=True)\n",
    "        shapefile_name = f\"ATL06_{track_id}_{name}_{date}.shp\"\n",
    "        atl06 = savepath / shapefile_name\n",
    "        gdf.to_file(atl06)\n",
    "        \n",
    "        # print(f\"Shapefile saved to {atl06}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\2025\\0281')\n",
    "files = list(path.glob('*.h5'))\n",
    "names = ['gt1r', 'gt1l', 'gt2r', 'gt2l', 'gt3r', 'gt3l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all files and names to create shapefiles for each group\n",
    "\n",
    "for file in files:\n",
    "    for name in names:\n",
    "        try:\n",
    "            ds = xr.open_dataset(file, group=f'/{name}/land_ice_segments', engine='h5netcdf')\n",
    "        except OSError:\n",
    "            print(f\"Group {name} in file {file} is empty or does not exist. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        lat = ds['latitude']\n",
    "        lon = ds['longitude']\n",
    "        h_li = ds['h_li']\n",
    "        \n",
    "        # Calculate the distance between each point\n",
    "        distance = np.zeros(lat.shape)\n",
    "        for i in range(1, len(lat)):\n",
    "            distance[i] = haversine_distance(lat[0], lon[0], lat[i], lon[i])\n",
    "        distance = distance * 1000  # convert to meters\n",
    "        \n",
    "        date = file.stem.split('_')[2][:8]\n",
    "        track_id = file.stem.split('_')[3][:4]\n",
    "        \n",
    "        data = {\n",
    "            'latitude': lat,\n",
    "            'longitude': lon,\n",
    "            'h_li': h_li,\n",
    "            'distance': distance,\n",
    "            'date': date,\n",
    "            'gt': name,\n",
    "            'track_id': track_id\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "        gdf.set_crs(epsg=4326, inplace=True)\n",
    "        \n",
    "        savepath = pl.Path(r'C:\\coding\\arctic\\paper1\\shp\\Colorado\\Extras')\n",
    "        savepath.mkdir(parents=True, exist_ok=True)\n",
    "        shapefile_name = f\"ATL06_{track_id}_{name}_{date}.shp\"\n",
    "        atl06 = savepath / shapefile_name\n",
    "        gdf.to_file(atl06)\n",
    "        \n",
    "        # print(f\"Shapefile saved to {atl06}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pl.Path(r'C:\\coding\\arctic\\paper1\\ATL06_all\\2025\\0167')\n",
    "files = list(path.glob('*.h5'))\n",
    "names = ['gt1r', 'gt1l', 'gt2r', 'gt2l', 'gt3r', 'gt3l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all files and names to create shapefiles for each group\n",
    "\n",
    "for file in files:\n",
    "    for name in names:\n",
    "        try:\n",
    "            ds = xr.open_dataset(file, group=f'/{name}/land_ice_segments', engine='h5netcdf')\n",
    "        except OSError:\n",
    "            print(f\"Group {name} in file {file} is empty or does not exist. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        lat = ds['latitude']\n",
    "        lon = ds['longitude']\n",
    "        h_li = ds['h_li']\n",
    "        \n",
    "        # Calculate the distance between each point\n",
    "        distance = np.zeros(lat.shape)\n",
    "        for i in range(1, len(lat)):\n",
    "            distance[i] = haversine_distance(lat[0], lon[0], lat[i], lon[i])\n",
    "        distance = distance * 1000  # convert to meters\n",
    "        \n",
    "        date = file.stem.split('_')[2][:8]\n",
    "        track_id = file.stem.split('_')[3][:4]\n",
    "        \n",
    "        data = {\n",
    "            'latitude': lat,\n",
    "            'longitude': lon,\n",
    "            'h_li': h_li,\n",
    "            'distance': distance,\n",
    "            'date': date,\n",
    "            'gt': name,\n",
    "            'track_id': track_id\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "        gdf.set_crs(epsg=4326, inplace=True)\n",
    "        \n",
    "        savepath = pl.Path(r'C:\\coding\\arctic\\paper1\\shp\\Colorado\\Extras')\n",
    "        savepath.mkdir(parents=True, exist_ok=True)\n",
    "        shapefile_name = f\"ATL06_{track_id}_{name}_{date}.shp\"\n",
    "        atl06 = savepath / shapefile_name\n",
    "        gdf.to_file(atl06)\n",
    "        \n",
    "        # print(f\"Shapefile saved to {atl06}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input shapefile path\n",
    "beam_shp_path = pl.Path(r\"C:\\coding\\arctic\\paper1\\DrewPoint\\ATL06\\0129\\shapefiles\\ATL06_0129_gt3r_20230329.shp\")\n",
    "\n",
    "# Step 1: Load shapefile (points)\n",
    "beam_gdf = gpd.read_file(beam_shp_path)\n",
    "\n",
    "# Step 2: Create LineString from point geometries\n",
    "beam_line = LineString(beam_gdf.geometry.tolist())\n",
    "line_gdf = gpd.GeoDataFrame({'geometry': [beam_line]}, crs='EPSG:4326')\n",
    "\n",
    "# Extract track and beam names from filename\n",
    "filename_parts = beam_shp_path.stem.split('_')\n",
    "track = filename_parts[1]\n",
    "beam = filename_parts[2]\n",
    "\n",
    "# Step 3: Save the LineString shapefile\n",
    "line_shp_name = f\"{track}_{beam}.shp\"\n",
    "line_shp_path = beam_shp_path.parent / line_shp_name\n",
    "line_gdf.to_file(line_shp_path)\n",
    "print(f\"‚úÖ LineString saved to: {line_shp_path}\")\n",
    "\n",
    "# Step 4: Project to metric CRS (EPSG:3413 for Arctic)\n",
    "line_gdf_proj = line_gdf.to_crs('EPSG:3413')\n",
    "\n",
    "# Step 5: Buffer the line (25 meters on each side)\n",
    "buffer_dist = 25\n",
    "buffered_proj = line_gdf_proj.buffer(buffer_dist)\n",
    "\n",
    "# Step 6: Convert buffer back to WGS84\n",
    "buffered_gdf = gpd.GeoDataFrame(geometry=buffered_proj, crs='EPSG:3413').to_crs('EPSG:4326')\n",
    "\n",
    "# Step 7: Save the buffer shapefile\n",
    "buffer_shp_name = f\"{track}_{beam}_buffer_{buffer_dist}.shp\"\n",
    "buffer_shp_path = beam_shp_path.parent / buffer_shp_name\n",
    "buffered_gdf.to_file(buffer_shp_path)\n",
    "print(f\"‚úÖ Buffer saved to: {buffer_shp_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select files that intersects buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Path to your buffer shapefile\n",
    "buffer_path = pl.Path(r\"C:\\coding\\arctic\\paper1\\DrewPoint\\ATL06\\0129\\shapefiles\\0129_gt3r_buffer_25.shp\")\n",
    "buffer_gdf = gpd.read_file(buffer_path)\n",
    "\n",
    "# Step 2: Folder where your other shapefiles are\n",
    "input_folder = buffer_path.parent\n",
    "\n",
    "# Step 3: Create output folder (if not exist)\n",
    "output_folder = input_folder / 'buffer_25'\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Step 4: Loop through all shapefiles in the folder\n",
    "for shp_file in input_folder.glob(\"*.shp\"):\n",
    "    if 'buffer' in shp_file.stem or 'line' in shp_file.stem:\n",
    "        continue  # Skip buffer or line shapefiles\n",
    "    \n",
    "    try:\n",
    "        gdf = gpd.read_file(shp_file)\n",
    "\n",
    "        # Ensure both are in the same CRS\n",
    "        if gdf.crs != buffer_gdf.crs:\n",
    "            gdf = gdf.to_crs(buffer_gdf.crs)\n",
    "\n",
    "        # Check for intersection\n",
    "        if gdf.geometry.intersects(buffer_gdf.geometry.iloc[0]).any():\n",
    "            # Copy all component files of the shapefile\n",
    "            base = shp_file.stem\n",
    "            for ext in ['.shp', '.shx', '.dbf', '.prj', '.cpg']:\n",
    "                file_to_copy = input_folder / f\"{base}{ext}\"\n",
    "                if file_to_copy.exists():\n",
    "                    shutil.copy2(file_to_copy, output_folder)\n",
    "            print(f\"‚úÖ Copied: {base}.*\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping {shp_file.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Buffer Folder to create plos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pl.Path(r'C:\\coding\\arctic\\paper1\\DrewPoint\\ATL06\\0129\\shapefiles\\buffer_25\\filtered_by_coastline')\n",
    "files = list(path.glob('*.shp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_files = list(path.glob('*.shp'))\n",
    "shp_file_count = len(shp_files)\n",
    "print(f\"Number of .shp files in the directory: {shp_file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# Create a single figure and axis for all files\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Iterate through each file in the files list\n",
    "for file in files:\n",
    "    df = gpd.read_file(file)\n",
    "\n",
    "    # Skip files with less than 30 points\n",
    "    if len(df) < 80:\n",
    "        print(f\"File {file.name} has less than 30 points.\")\n",
    "        continue\n",
    "\n",
    "    # Skip empty dataframes\n",
    "    if df.empty:\n",
    "        print(f\"File {file.name} is empty after dropping NaN values.\")\n",
    "        continue\n",
    "\n",
    "    # Filter h_li values within a specific range\n",
    "    h_li_min, h_li_max = -2.0, 10.0  # Define the range\n",
    "    df = df[(df['h_li'] >= h_li_min) & (df['h_li'] <= h_li_max)]\n",
    "    if df.empty:\n",
    "        print(f\"File {file.name} has no h_li values within the range {h_li_min} to {h_li_max}.\")\n",
    "        continue\n",
    "\n",
    "    # Plot line\n",
    "    ax.plot(df['distance'], df['h_li'], label=file.stem, linewidth=1)\n",
    "\n",
    "    # Plot scatter\n",
    "    ax.scatter(df['distance'], df['h_li'], s=2)\n",
    "\n",
    "# Add labels, grid, and legend\n",
    "ax.set_xlabel('Distance (m)')\n",
    "ax.set_xlim(2200, 2500)\n",
    "ax.set_ylabel('h_li (m)')\n",
    "ax.set_title('ICESat-2 Elevation Profiles')\n",
    "ax.grid(True)\n",
    "ax.legend(fontsize='small', loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "\n",
    "# Show the combined plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a single figure and axis for all files\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Iterate through each file in the files list\n",
    "for file in files:\n",
    "    df = gpd.read_file(file)\n",
    "\n",
    "    # Skip files with less than 30 points\n",
    "    if len(df) < 15:\n",
    "        print(f\"File {file.name} has less than 15 points.\")\n",
    "        continue\n",
    "\n",
    "    # Skip empty dataframes\n",
    "    if df.empty:\n",
    "        print(f\"File {file.name} is empty after dropping NaN values.\")\n",
    "        continue\n",
    "\n",
    "    # Filter h_li values within a specific range\n",
    "    h_li_min, h_li_max = -3.0, 10.0  # Define the range\n",
    "    df = df[(df['h_li'] >= h_li_min) & (df['h_li'] <= h_li_max)]\n",
    "    if df.empty:\n",
    "        print(f\"File {file.name} has no h_li values within the range {h_li_min} to {h_li_max}.\")\n",
    "        continue\n",
    "\n",
    "    # Plot line\n",
    "    ax.plot(df['distance'], df['h_li'], label=file.stem, linewidth=1)\n",
    "\n",
    "    # Plot scatter\n",
    "    ax.scatter(df['distance'], df['h_li'], s=2)\n",
    "\n",
    "    # Annotate points with their index\n",
    "    for idx, row in df.iterrows():\n",
    "        ax.annotate(idx, (row['distance'], row['h_li']), fontsize=6, alpha=0.7)\n",
    "\n",
    "# Add a vertical line at distance 458\n",
    "plt.axvline(x=2374, color='blue', linestyle='--', label='Aproximate Coastline')\n",
    "\n",
    "# Add labels, grid, and legend\n",
    "ax.set_xlabel('Distance (m)')\n",
    "ax.set_xlim(2200, 2650)\n",
    "ax.set_ylabel('h_li (m)')\n",
    "ax.set_title('ICESat-2 Elevation Profiles')\n",
    "ax.grid(True)\n",
    "ax.legend(fontsize='small', loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "\n",
    "# Show the combined plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a single figure and axis for all files\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Iterate through each file in the files list\n",
    "for file in files:\n",
    "    df = gpd.read_file(file)\n",
    "\n",
    "    # Skip files with less than 30 points\n",
    "    if len(df) < 80:\n",
    "        print(f\"File {file.name} has less than 80 points.\")\n",
    "        continue\n",
    "\n",
    "    # Skip empty dataframes\n",
    "    if df.empty:\n",
    "        print(f\"File {file.name} is empty.\")\n",
    "        continue\n",
    "\n",
    "    # Filter h_li values within a specific range\n",
    "    h_li_min, h_li_max = -2.0, 10.0\n",
    "    df = df[(df['h_li'] >= h_li_min) & (df['h_li'] <= h_li_max)]\n",
    "    if df.empty:\n",
    "        print(f\"File {file.name} has no h_li values within the range {h_li_min} to {h_li_max}.\")\n",
    "        continue\n",
    "\n",
    "    # Ensure CRS is defined\n",
    "    if df.crs is None:\n",
    "        df.set_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "    # Reproject to a metric CRS (EPSG:3413)\n",
    "    gdf_proj = df.to_crs(\"EPSG:3413\")\n",
    "\n",
    "    # Extract projected x-coordinate (in meters)\n",
    "    df['x_meters'] = gdf_proj.geometry.x\n",
    "\n",
    "    # Plot using projected x-coordinates\n",
    "    ax.plot(df['x_meters'], df['h_li'], label=file.stem, linewidth=1)\n",
    "    ax.scatter(df['x_meters'], df['h_li'], s=2)\n",
    "\n",
    "# Add labels, grid, and legend\n",
    "ax.set_xlabel('X Coordinate (meters, EPSG:3413)')\n",
    "ax.set_ylabel('h_li (m)')\n",
    "# ax.set_xlim(400000, 405000)\n",
    "ax.set_title('ICESat-2 Elevation Profiles (Aligned by Location)')\n",
    "ax.grid(True)\n",
    "ax.legend(fontsize='small', loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "coastline_path = pl.Path(r\"C:\\coding\\arctic\\Gis\\datasets\\MyOwn\\AOI_shoreline.shp\")        # the red line\n",
    "points_path = pl.Path(r'C:\\coding\\arctic\\paper1\\DrewPoint\\ATL06\\0129\\cluster')      # your ICESat-2 beam points\n",
    "\n",
    "# Load coastline and ICESat-2 points\n",
    "coastline = gpd.read_file(coastline_path)\n",
    "points = gpd.read_file(points_path)\n",
    "\n",
    "# Reproject to a metric CRS (e.g., EPSG:3413 for Arctic)\n",
    "coastline = coastline.to_crs(\"EPSG:3413\")\n",
    "points = points.to_crs(\"EPSG:3413\")\n",
    "\n",
    "# Create 200 m buffer (100 m onshore + 200 m offshore)\n",
    "coast_buffer = coastline.buffer(300)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "buffer_gdf = gpd.GeoDataFrame(geometry=coast_buffer, crs=coastline.crs)\n",
    "\n",
    "# Spatial join: select points within the buffer\n",
    "points_within_buffer = points[points.geometry.within(buffer_gdf.unary_union)]\n",
    "\n",
    "# Save selected points\n",
    "output_path = points_path.parent / f\"{points_path.stem}_coastline_filtered.shp\"\n",
    "points_within_buffer.to_file(output_path)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(points_within_buffer)} points near coastline to:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "import pathlib as pl\n",
    "\n",
    "# === USER INPUTS ===\n",
    "input_folder = pl.Path(r\"C:\\coding\\arctic\\paper1\\Utqiagvik\\ATL06\\shapefile\\Nasa1265\\cluster\")         # Folder with ICESat-2 point shapefiles\n",
    "coastline_path = pl.Path(r\"C:\\coding\\arctic\\Gis\\datasets\\MyOwn\\AOI_shoreline.shp\")          # Coastline shapefile (LineString)\n",
    "output_folder = input_folder / \"filtered\"      # Output folder\n",
    "\n",
    "buffer_dist = 300  # 200 meters on each side = 200m total width\n",
    "\n",
    "# === PREP ===\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "coastline = gpd.read_file(coastline_path).to_crs(\"EPSG:3413\")\n",
    "coast_buffer = coastline.buffer(buffer_dist)\n",
    "coast_buffer_union = gpd.GeoSeries(coast_buffer.union_all(), crs=coastline.crs)\n",
    "\n",
    "# === PROCESS EACH FILE ===\n",
    "for shp_file in input_folder.glob(\"*.shp\"):\n",
    "    if \"buffer\" in shp_file.stem.lower() or \"line\" in shp_file.stem.lower():\n",
    "        continue  # Skip buffer/line files if they're in the same folder\n",
    "\n",
    "    try:\n",
    "        # Load and reproject\n",
    "        points = gpd.read_file(shp_file).to_crs(\"EPSG:3413\")\n",
    "\n",
    "        # Select points within the buffer\n",
    "        points_selected = points[points.geometry.within(coast_buffer_union[0])]\n",
    "\n",
    "        if not points_selected.empty:\n",
    "            # Save filtered points\n",
    "            out_path = output_folder / f\"{shp_file.stem}.shp\"\n",
    "            points_selected.to_file(out_path)\n",
    "            print(f\"‚úÖ {shp_file.name}: {len(points_selected)} points saved.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {shp_file.name}: no points within 200m buffer.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to process {shp_file.name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
