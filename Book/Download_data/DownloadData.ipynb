{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "042b1151",
   "metadata": {},
   "source": [
    "## Two option to download data from ICESat-2\n",
    "\n",
    "- It is working on 11.11.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcccd98",
   "metadata": {},
   "source": [
    "# Option 1:\n",
    "\n",
    "- download all tracks in a AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de757a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOI exterior vertices used: 5\n",
      "Found 38 ATL03 granule(s) with polygon filter.\n"
     ]
    }
   ],
   "source": [
    "# --- ICESat-2 download via earthaccess with 2D/3D-safe AOI polygon ---\n",
    "# pip install earthaccess geopandas shapely pyproj fiona\n",
    "\n",
    "import os, json\n",
    "import geopandas as gpd\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Polygon, MultiPolygon, LineString, Point, mapping\n",
    "from shapely.validation import make_valid\n",
    "import earthaccess as ea\n",
    "\n",
    "# ------------------ USER SETTINGS ------------------\n",
    "SHAPEFILE_PATH = r\"C:\\coding\\arctic\\paper1\\shp\\AOI_Drew.shp\"\n",
    "OUTPUT_DIR     = r\"C:\\coding\\arctic\\data\"\n",
    "SHORT_NAME     = \"ATL03\"      # or ATL03/ATL08/ATL12...\n",
    "VERSION        = \"007\"        # use \"007\" for current ATL06; or None\n",
    "TIME_START     = \"2020-01-01\"\n",
    "TIME_END       = \"2020-11-10\"\n",
    "CLOUD_HOSTED   = True         # set False to use NSIDC Direct instead of AWS cloud host\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def to_single_polygon(g):\n",
    "    \"\"\"Coerce any geometry into a single valid Polygon for CMR.\"\"\"\n",
    "    if g.is_empty:\n",
    "        raise ValueError(\"AOI geometry is empty.\")\n",
    "    if isinstance(g, Polygon):\n",
    "        poly = g\n",
    "    elif isinstance(g, MultiPolygon):\n",
    "        poly = max(g.geoms, key=lambda x: x.area)\n",
    "    elif isinstance(g, LineString):\n",
    "        poly = g.buffer(0.001)  # ~100 m at equator; AOI just needs a footprint\n",
    "    elif isinstance(g, Point):\n",
    "        poly = g.buffer(0.001)\n",
    "    else:\n",
    "        poly = g.convex_hull\n",
    "    # fix self-intersections etc.\n",
    "    poly = make_valid(poly)\n",
    "    if isinstance(poly, MultiPolygon):\n",
    "        poly = max(poly.geoms, key=lambda x: x.area)\n",
    "    if not isinstance(poly, Polygon) or poly.is_empty:\n",
    "        raise ValueError(\"Could not produce a valid polygon from AOI.\")\n",
    "    return poly\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry.polygon import orient\n",
    "\n",
    "def cmr_polygon_coords(poly, max_points=500):\n",
    "    \"\"\"\n",
    "    Return exterior ring as list of [lon, lat] PAIRS for CMR, CLOSED (last==first),\n",
    "    explicitly oriented counter-clockwise. Handles 2D/3D coords; simplifies if needed.\n",
    "    \"\"\"\n",
    "    if not isinstance(poly, Polygon):\n",
    "        raise ValueError(\"AOI is not a Polygon.\")\n",
    "\n",
    "    # 1) Force CCW orientation\n",
    "    poly_ccw = orient(poly, sign=1.0)\n",
    "\n",
    "    # 2) Simplify if vertex count too high, then enforce CCW again (simplify can flip it)\n",
    "    candidate = poly_ccw.simplify(0.0002, preserve_topology=True) \\\n",
    "                if len(poly_ccw.exterior.coords) > max_points else poly_ccw\n",
    "    candidate = orient(candidate, sign=1.0)\n",
    "\n",
    "    ring = candidate.exterior\n",
    "\n",
    "    # 3) Build lon/lat pairs (support 2D/3D), normalize longitudes\n",
    "    coords = []\n",
    "    for c in ring.coords:\n",
    "        x = float(c[0]); y = float(c[1])\n",
    "        if x > 180: x -= 360\n",
    "        if x < -180: x += 360\n",
    "        coords.append((x, y))\n",
    "\n",
    "    # 4) Close the ring (CMR requires last==first) and basic checks\n",
    "    if len(coords) < 4:\n",
    "        raise ValueError(f\"CMR polygon needs >= 4 coordinate pairs; got {len(coords)}.\")\n",
    "    if coords[0] != coords[-1]:\n",
    "        coords.append(coords[0])\n",
    "    if len(coords) < 5:\n",
    "        raise ValueError(\"Closed polygon must contain at least 4 unique vertices (5 with closure).\")\n",
    "\n",
    "    return coords\n",
    "\n",
    "\n",
    "# 1) Read AOI and ensure WGS84\n",
    "gdf = gpd.read_file(SHAPEFILE_PATH)\n",
    "if gdf.crs is None:\n",
    "    raise ValueError(\"Shapefile has no CRS. Define/reproject it, then retry.\")\n",
    "if gdf.crs.to_epsg() != 4326:\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# 2) Merge into a single polygon\n",
    "geom = unary_union(gdf.geometry)\n",
    "poly = to_single_polygon(geom)\n",
    "coords = cmr_polygon_coords(poly)\n",
    "print(f\"AOI exterior vertices used: {len(coords)}\")\n",
    "\n",
    "# 3) Auth & search\n",
    "ea.login(strategy=\"netrc\")  # prompts if no ~/.netrc\n",
    "kwargs = {\n",
    "    \"short_name\": SHORT_NAME,\n",
    "    \"temporal\": (TIME_START, TIME_END),\n",
    "    \"polygon\": coords,              # <-- CMR expects a flat list of lon/lat pairs\n",
    "    \"cloud_hosted\": CLOUD_HOSTED\n",
    "}\n",
    "if VERSION is not None:\n",
    "    kwargs[\"version\"] = VERSION\n",
    "\n",
    "results = ea.search_data(**kwargs)\n",
    "print(f\"Found {len(results)} {SHORT_NAME} granule(s) with polygon filter.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb7a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Download (fallback to bbox if polygon returned none)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "if results:\n",
    "    downloads = ea.download(results, local_path=OUTPUT_DIR)\n",
    "    print(f\"Downloaded {len(downloads)} file(s) to: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    minx, miny, maxx, maxy = poly.bounds\n",
    "    print(\"No polygon results; retrying with bounding box…\")\n",
    "    bb_args = {\n",
    "        \"short_name\": SHORT_NAME,\n",
    "        \"temporal\": (TIME_START, TIME_END),\n",
    "        \"bounding_box\": (minx, miny, maxx, maxy),\n",
    "        \"cloud_hosted\": CLOUD_HOSTED\n",
    "    }\n",
    "    if VERSION is not None:\n",
    "        bb_args[\"version\"] = VERSION\n",
    "    results_bb = ea.search_data(**bb_args)\n",
    "    print(f\"Found {len(results_bb)} with bounding box.\")\n",
    "    if results_bb:\n",
    "        downloads = ea.download(results_bb, local_path=OUTPUT_DIR, flatten=True)\n",
    "        print(f\"Downloaded {len(downloads)} file(s) to: {OUTPUT_DIR}\")\n",
    "\n",
    "# 5) Save query metadata\n",
    "meta = {\n",
    "    \"short_name\": SHORT_NAME,\n",
    "    \"version\": VERSION,\n",
    "    \"temporal\": {\"start\": TIME_START, \"end\": TIME_END},\n",
    "    \"polygon_coords_len\": len(coords),\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"cloud_hosted\": CLOUD_HOSTED\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, f\"{SHORT_NAME}_download_query.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Saved query metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179d066",
   "metadata": {},
   "source": [
    "# Option 2:\n",
    "\n",
    "- Download a selected track(or tracks) for a AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4600c904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOI vertices: 5 | Bounds: (-153.93582877221303, 70.872503521221, -153.673676720986, 70.89384248884699)\n",
      "Found 5 ATL03 granule(s) before track filtering.\n",
      "  • ATL03_20190203200043_05710203_007_01.h5\n",
      "  • ATL03_20190308182822_10740203_007_01.h5\n",
      "  • ATL03_20190309071315_10820205_007_01.h5\n",
      "  • ATL03_20190406170422_01290303_007_01.h5\n",
      "  • ATL03_20190407054915_01370305_007_01.h5\n",
      "Requested tracks: ['0137']\n",
      "Parsed tracks found: ['0129', '0137', '0571', '1074', '1082']  |  unknown=0\n",
      "Downloading 1 track(s): ['0137']\n",
      "  Track 0137: 1 file(s)\n"
     ]
    }
   ],
   "source": [
    "# --- ICESat-2 download via earthaccess with AOI shapefile + multi-track filtering ---\n",
    "# Install once (separate cell / terminal):\n",
    "#   pip install earthaccess geopandas shapely pyproj fiona\n",
    "\n",
    "import os, re, json\n",
    "import geopandas as gpd\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Polygon, MultiPolygon, LineString, Point\n",
    "from shapely.validation import make_valid\n",
    "from shapely.geometry.polygon import orient\n",
    "import earthaccess as ea\n",
    "\n",
    "# ===================== USER SETTINGS =====================\n",
    "SHAPEFILE_PATH = r\"C:\\coding\\arctic\\paper1\\shp\\AOI_Drew_CLEAN.shp\"\n",
    "OUTPUT_DIR     = r\"C:\\coding\\arctic\\paper1\\ATL06_all\\ATL03\\0129\"\n",
    "\n",
    "SHORT_NAME     = \"ATL03\"      # e.g., ATL03 (photons), ATL06 (heights), ATL08, ATL12\n",
    "VERSION        = \"007\"        # set None to accept any (ATL06 usually \"007\")\n",
    "TIME_START     = \"2019-02-01\"\n",
    "TIME_END       = \"2019-04-30\"\n",
    "CLOUD_HOSTED   = True         # True: prefer AWS cloud-hosted; False: NSIDC Direct\n",
    "\n",
    "# —— Choose tracks ——\n",
    "# Use None to download ALL tracks that intersect your AOI/time, or a list like [\"129\",\"731\"]\n",
    "TRACKS         = [\"137\"]    # <- example; ints or strings OK; they’ll be zero-padded to 4 digits\n",
    "SPLIT_BY_TRACK = True              # save each track to its own subfolder under OUTPUT_DIR\n",
    "# =========================================================\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def to_single_polygon(g):\n",
    "    \"\"\"Coerce any geometry into a single valid Polygon for CMR.\"\"\"\n",
    "    if g is None or g.is_empty:\n",
    "        raise ValueError(\"AOI geometry is empty.\")\n",
    "    if isinstance(g, Polygon):\n",
    "        poly = g\n",
    "    elif isinstance(g, MultiPolygon):\n",
    "        poly = max(g.geoms, key=lambda x: x.area)\n",
    "    elif isinstance(g, LineString):\n",
    "        poly = g.buffer(0.001)  # ~100 m at equator\n",
    "    elif isinstance(g, Point):\n",
    "        poly = g.buffer(0.001)\n",
    "    else:\n",
    "        poly = g.convex_hull\n",
    "    poly = make_valid(poly)\n",
    "    if isinstance(poly, MultiPolygon):\n",
    "        poly = max(poly.geoms, key=lambda x: x.area)\n",
    "    if not isinstance(poly, Polygon) or poly.is_empty:\n",
    "        raise ValueError(\"Could not produce a valid polygon from AOI.\")\n",
    "    return poly\n",
    "\n",
    "def cmr_polygon_coords(poly, max_points=500):\n",
    "    \"\"\"\n",
    "    Return exterior ring as list of [lon, lat] pairs for CMR, CLOSED and CCW.\n",
    "    Handles 2D/3D coords and simplifies if too many vertices.\n",
    "    \"\"\"\n",
    "    if not isinstance(poly, Polygon):\n",
    "        raise ValueError(\"AOI is not a Polygon.\")\n",
    "    # Force CCW orientation\n",
    "    poly = orient(poly, sign=1.0)\n",
    "    # Simplify if too many vertices; enforce CCW again (simplify may flip)\n",
    "    if len(poly.exterior.coords) > max_points:\n",
    "        poly = orient(poly.simplify(0.0002, preserve_topology=True), sign=1.0)\n",
    "\n",
    "    coords = []\n",
    "    for c in poly.exterior.coords:\n",
    "        x = float(c[0]); y = float(c[1])\n",
    "        # normalize longitudes to [-180, 180] just in case\n",
    "        if x > 180: x -= 360\n",
    "        if x < -180: x += 360\n",
    "        coords.append((x, y))\n",
    "\n",
    "    if len(coords) < 4:\n",
    "        raise ValueError(f\"CMR polygon needs >= 4 coordinate pairs; got {len(coords)}.\")\n",
    "    # close ring (CMR requires last==first)\n",
    "    if coords[0] != coords[-1]:\n",
    "        coords.append(coords[0])\n",
    "    if len(coords) < 5:\n",
    "        raise ValueError(\"Closed polygon must contain at least 4 unique vertices (5 with closure).\")\n",
    "    return coords\n",
    "\n",
    "def granule_name(g):\n",
    "    \"\"\"Prefer ProducerGranuleId or GranuleUR; fallback to any id-like string.\"\"\"\n",
    "    umm = getattr(g, \"umm\", None) or (g.get(\"umm\", {}) if isinstance(g, dict) else {})\n",
    "    if isinstance(umm, dict):\n",
    "        dg  = umm.get(\"DataGranule\", {}) or {}\n",
    "        pid = dg.get(\"ProducerGranuleId\")\n",
    "        if pid: return str(pid)\n",
    "        gur = umm.get(\"GranuleUR\")\n",
    "        if gur: return str(gur)\n",
    "    return str(getattr(g, \"granule_id\", \"\") or getattr(g, \"id\", \"\") or g)\n",
    "\n",
    "def extract_track_from_name(name: str):\n",
    "    \"\"\"\n",
    "    Get the 4-digit RGT from an ICESat-2 granule name.\n",
    "    Prefer underscore-splitting (robust), then regex fallback.\n",
    "    Examples:\n",
    "      ATL03_20190105212430_01290203_007_01.h5  -> 0129\n",
    "      ATL06_20230714T101530_07310115_007_02.h5 -> 0731\n",
    "    \"\"\"\n",
    "    s = str(name)\n",
    "    parts = s.split(\"_\")\n",
    "    # prefer 8-digit (RGT+cycle) block: take first 4 digits as RGT\n",
    "    for p in parts:\n",
    "        if p.isdigit() and len(p) == 8:\n",
    "            return p[:4]\n",
    "    # then plain 4-digit block\n",
    "    for p in parts:\n",
    "        if p.isdigit() and len(p) == 4:\n",
    "            return p\n",
    "    # regex fallback (odd separators)\n",
    "    m = re.search(r\"[_.-](\\d{8})[_.-]\", s)\n",
    "    if m: return m.group(1)[:4]\n",
    "    m = re.search(r\"[_.-](\\d{4})[_.-]\", s)\n",
    "    if m: return m.group(1)\n",
    "    return None\n",
    "\n",
    "def extract_track(g):\n",
    "    \"\"\"Extract 4-digit RGT from a granule object using the filename.\"\"\"\n",
    "    rgt = extract_track_from_name(granule_name(g))\n",
    "    return rgt.zfill(4) if rgt else None\n",
    "\n",
    "def z4(x): \n",
    "    return str(x).zfill(4)\n",
    "\n",
    "# ---------------- Main ----------------\n",
    "# 1) Read AOI and ensure WGS84\n",
    "gdf = gpd.read_file(SHAPEFILE_PATH)\n",
    "if gdf.crs is None:\n",
    "    raise ValueError(\"Shapefile has no CRS. Define/reproject it, then retry.\")\n",
    "if gdf.crs.to_epsg() != 4326:\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# 2) AOI -> polygon -> coords\n",
    "geom = unary_union(gdf.geometry)\n",
    "poly = to_single_polygon(geom)\n",
    "coords = cmr_polygon_coords(poly)\n",
    "print(f\"AOI vertices: {len(coords)} | Bounds: {poly.bounds}\")\n",
    "\n",
    "# 3) Auth & search\n",
    "ea.login(strategy=\"netrc\")  # prompts if not configured\n",
    "\n",
    "kwargs = {\n",
    "    \"short_name\": SHORT_NAME,\n",
    "    \"temporal\": (TIME_START, TIME_END),\n",
    "    \"polygon\": coords,\n",
    "    \"cloud_hosted\": CLOUD_HOSTED\n",
    "}\n",
    "if VERSION is not None:\n",
    "    kwargs[\"version\"] = VERSION\n",
    "\n",
    "# 4) Search by polygon; fallback to bbox if needed\n",
    "try:\n",
    "    results = ea.search_data(**kwargs)\n",
    "except Exception as e:\n",
    "    print(f\"Polygon search raised: {e}\\nFalling back to bounding box…\")\n",
    "    minx, miny, maxx, maxy = poly.bounds\n",
    "    kwargs_bb = {\n",
    "        \"short_name\": SHORT_NAME,\n",
    "        \"temporal\": (TIME_START, TIME_END),\n",
    "        \"bounding_box\": (minx, miny, maxx, maxy),\n",
    "        \"cloud_hosted\": CLOUD_HOSTED\n",
    "    }\n",
    "    if VERSION is not None:\n",
    "        kwargs_bb[\"version\"] = VERSION\n",
    "    results = ea.search_data(**kwargs_bb)\n",
    "\n",
    "print(f\"Found {len(results)} {SHORT_NAME} granule(s) before track filtering.\")\n",
    "for g in results[:5]:\n",
    "    print(\"  •\", granule_name(g))\n",
    "\n",
    "# 5) Multi-track filter & download\n",
    "requested = None if (not TRACKS) else {z4(t) for t in TRACKS}\n",
    "print(\"Requested tracks:\", sorted(requested) if requested else \"(all)\")\n",
    "\n",
    "# Parse RGT for each result\n",
    "by_track = {}\n",
    "unknown = []\n",
    "for g in results:\n",
    "    rgt = extract_track(g)  # returns 4-digit string or None\n",
    "    if rgt is None:\n",
    "        unknown.append(g)\n",
    "    else:\n",
    "        by_track.setdefault(rgt, []).append(g)\n",
    "\n",
    "print(f\"Parsed tracks found: {sorted(by_track.keys())}  |  unknown={len(unknown)}\")\n",
    "\n",
    "# Decide batches to download\n",
    "if not requested:\n",
    "    batches = {\"ALL\": results}  # no filtering\n",
    "else:\n",
    "    batches = {rgt: by_track.get(rgt, []) for rgt in requested}\n",
    "    missing = [r for r in requested if not batches[r]]\n",
    "    if missing:\n",
    "        print(\"No granules for tracks:\", sorted(missing))\n",
    "\n",
    "# print how many tracks to download\n",
    "print(f\"Downloading {len(batches)} track(s):\", sorted(batches.keys()))\n",
    "\n",
    "# print number of files per track\n",
    "for rgt, granules in batches.items():\n",
    "    print(f\"  Track {rgt}: {len(granules)} file(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d77322f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8786af0cb780466f834aa3d3cb052e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fdd2a46826d4ef2a70941416e17097e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc0f500bf1544de96a4ddef56cdd701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1 file(s) for track 0137 → C:\\coding\\arctic\\paper1\\ATL06_all\\ATL03\\0129\\0137\n",
      "Saved query metadata.\n"
     ]
    }
   ],
   "source": [
    "# 6) Download\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "total = 0\n",
    "for rgt, granules in batches.items():\n",
    "    if not granules:\n",
    "        continue\n",
    "    subdir = os.path.join(OUTPUT_DIR, f\"{rgt}\") if SPLIT_BY_TRACK and rgt != \"ALL\" else OUTPUT_DIR\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "    try:\n",
    "        dl = ea.download(granules, local_path=subdir, keep_structure=False)\n",
    "    except TypeError:\n",
    "        dl = ea.download(granules, local_path=subdir)\n",
    "    print(f\"Downloaded {len(dl)} file(s) for track {rgt} → {subdir}\")\n",
    "    total += len(dl)\n",
    "\n",
    "if total == 0:\n",
    "    print(\"No granules to download. Consider adjusting time window, version, AOI, or requested tracks.\")\n",
    "\n",
    "# 7) Save query metadata\n",
    "meta = {\n",
    "    \"short_name\": SHORT_NAME,\n",
    "    \"version\": VERSION,\n",
    "    \"temporal\": {\"start\": TIME_START, \"end\": TIME_END},\n",
    "    \"polygon_coords_len\": len(coords),\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"cloud_hosted\": CLOUD_HOSTED,\n",
    "    \"tracks_requested\": (sorted(requested) if requested else \"(all)\"),\n",
    "    \"tracks_found\": sorted(by_track.keys())\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, f\"{SHORT_NAME}_download_query.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Saved query metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e0239",
   "metadata": {},
   "source": [
    "## New version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60e299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 raw ATL03 files\n"
     ]
    }
   ],
   "source": [
    "import pathlib as pl\n",
    "\n",
    "OUTPUT_DIR_RAW = r\"C:\\coding\\arctic\\paper1\\ATL06_all\\ATL03\\0129\\0129\"\n",
    "files = list(pl.Path(OUTPUT_DIR_RAW).glob(\"*.h5\"))\n",
    "\n",
    "print(\"Found\", len(files), \"raw ATL03 files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21b5eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import Point\n",
    "# from shapely.vectorized import contains\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # INPUT: same AOI from your old cell\n",
    "# # AOI = polygon from the old download cell\n",
    "\n",
    "# CLIPPED_DIR = r\"C:\\coding\\arctic\\paper1\\ATL06_all\\ATL03_clipped\"\n",
    "# os.makedirs(CLIPPED_DIR, exist_ok=True)\n",
    "\n",
    "# BEAMS = [\"gt1l\",\"gt1r\",\"gt2l\",\"gt2r\",\"gt3l\",\"gt3r\"]\n",
    "\n",
    "# minx, miny, maxx, maxy = AOI.bounds\n",
    "\n",
    "# def clip_one(path):\n",
    "#     print(f\"\\nClipping: {os.path.basename(path)}\")\n",
    "\n",
    "#     with h5py.File(path, \"r\") as f:\n",
    "#         for beam in BEAMS:\n",
    "#             grp = f.get(f\"{beam}/heights\")\n",
    "#             if grp is None:\n",
    "#                 continue\n",
    "\n",
    "#             lat = grp[\"lat_ph\"][:]\n",
    "#             lon = grp[\"lon_ph\"][:]\n",
    "#             h   = grp[\"h_ph\"][:]\n",
    "#             conf = grp[\"signal_conf_ph\"][:]\n",
    "\n",
    "#             # Fast bounding box filter\n",
    "#             m1 = (\n",
    "#                 (lon >= minx) & (lon <= maxx) &\n",
    "#                 (lat >= miny) & (lat <= maxy)\n",
    "#             )\n",
    "#             if m1.sum() == 0:\n",
    "#                 continue\n",
    "\n",
    "#             lon2, lat2 = lon[m1], lat[m1]\n",
    "#             h2         = h[m1]\n",
    "#             conf2      = conf[m1]\n",
    "\n",
    "#             # Accurate polygon filter\n",
    "#             m2 = contains(AOI, lon2, lat2)\n",
    "#             if m2.sum() == 0:\n",
    "#                 continue\n",
    "\n",
    "#             gdf = gpd.GeoDataFrame(\n",
    "#                 {\n",
    "#                     \"longitude\": lon2[m2],\n",
    "#                     \"latitude\":  lat2[m2],\n",
    "#                     \"h_ph\":      h2[m2],\n",
    "#                     \"signal_conf\": list(conf2[m2]),\n",
    "#                     \"beam\": beam,\n",
    "#                     \"granule\": os.path.basename(path)\n",
    "#                 },\n",
    "#                 geometry=[Point(x, y) for x, y in zip(lon2[m2], lat2[m2])],\n",
    "#                 crs=\"EPSG:4326\"\n",
    "#             )\n",
    "\n",
    "#             outfile = os.path.basename(path).replace(\".h5\", f\"_{beam}.parquet\")\n",
    "#             gdf.to_parquet(os.path.join(CLIPPED_DIR, outfile))\n",
    "\n",
    "#             print(f\"   saved {outfile} ({len(gdf)} photons)\")\n",
    "\n",
    "\n",
    "# # Run clipping for all downloaded files\n",
    "# for f in files:   # <-- 'files' comes from your OLD download cell\n",
    "#     clip_one(f)\n",
    "# # ============================================================\n",
    "# # CLIP ATL03 PHOTONS TO AOI — Save as .parquet AND simplified .h5\n",
    "# # ============================================================\n",
    "\n",
    "# import pathlib as pl\n",
    "# import h5py\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import Point\n",
    "# from shapely.vectorized import contains\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 1) Load AOI polygon (this must already exist from old cell)\n",
    "# # ------------------------------------------------------------\n",
    "# # AOI = <polygon from download cell>\n",
    "# minx, miny, maxx, maxy = AOI.bounds\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 2) Load downloaded RAW ATL03 files from disk\n",
    "# # ------------------------------------------------------------\n",
    "# OUTPUT_DIR_RAW = pl.Path(r\"C:\\coding\\arctic\\paper1\\ATL06_all\\ATL03\\0129\\0129\")\n",
    "# files = list(OUTPUT_DIR_RAW.glob(\"*.h5\"))\n",
    "# print(\"Found\", len(files), \"raw ATL03 files\")\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 3) Output directories\n",
    "# # ------------------------------------------------------------\n",
    "# CLIPPED_PARQUET_DIR = pl.Path(r\"C:\\coding\\arctic\\paper1\\ATL06_all\\ATL03_clipped_parquet\")\n",
    "# CLIPPED_H5_DIR      = pl.Path(r\"C:\\coding\\arctic\\paper1\\ATL06_all\\ATL03_clipped_h5\")\n",
    "\n",
    "# CLIPPED_PARQUET_DIR.mkdir(exist_ok=True)\n",
    "# CLIPPED_H5_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 4) Beams\n",
    "# # ------------------------------------------------------------\n",
    "# BEAMS = [\"gt1l\",\"gt1r\",\"gt2l\",\"gt2r\",\"gt3l\",\"gt3r\"]\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 5) Function to write simplified clipped beam to HDF5\n",
    "# # ------------------------------------------------------------\n",
    "# def save_to_h5(gdf, granule_name, beam):\n",
    "#     \"\"\"\n",
    "#     Save clipped photons to a simplified HDF5 file with structure:\n",
    "#        /beam\n",
    "#            lat_ph\n",
    "#            lon_ph\n",
    "#            h_ph\n",
    "#            signal_conf_ph\n",
    "#     \"\"\"\n",
    "#     h5_path = CLIPPED_H5_DIR / f\"{granule_name}_{beam}.h5\"\n",
    "\n",
    "#     with h5py.File(h5_path, \"w\") as h5:\n",
    "#         grp = h5.create_group(beam)\n",
    "#         grp.create_dataset(\"lat_ph\", data=gdf[\"latitude\"].values)\n",
    "#         grp.create_dataset(\"lon_ph\", data=gdf[\"longitude\"].values)\n",
    "#         grp.create_dataset(\"h_ph\", data=gdf[\"h_ph\"].values)\n",
    "\n",
    "#         # Convert list-of-arrays to 2D numpy array for HDF5\n",
    "#         conf_arr = np.vstack(gdf[\"signal_conf\"].values)\n",
    "#         grp.create_dataset(\"signal_conf_ph\", data=conf_arr)\n",
    "\n",
    "#     print(f\"   Saved HDF5: {h5_path.name}\")\n",
    "\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 6) Main clip function\n",
    "# # ------------------------------------------------------------\n",
    "# def clip_one(path):\n",
    "#     print(f\"\\nClipping: {os.path.basename(path)}\")\n",
    "#     granule_name = os.path.basename(path).replace(\".h5\",\"\")\n",
    "\n",
    "#     with h5py.File(path, \"r\") as f:\n",
    "\n",
    "#         for beam in BEAMS:\n",
    "#             grp = f.get(f\"{beam}/heights\")\n",
    "#             if grp is None:\n",
    "#                 continue\n",
    "\n",
    "#             # Load ATL03 photon data\n",
    "#             lat  = grp[\"lat_ph\"][:]\n",
    "#             lon  = grp[\"lon_ph\"][:]\n",
    "#             h    = grp[\"h_ph\"][:]\n",
    "#             conf = grp[\"signal_conf_ph\"][:]\n",
    "\n",
    "#             # -----------------------------\n",
    "#             # Step 1: Fast bounding-box mask\n",
    "#             # -----------------------------\n",
    "#             m1 = (\n",
    "#                 (lon >= minx) & (lon <= maxx) &\n",
    "#                 (lat >= miny) & (lat <= maxy)\n",
    "#             )\n",
    "#             if m1.sum() == 0:\n",
    "#                 continue\n",
    "\n",
    "#             lat2, lon2 = lat[m1], lon[m1]\n",
    "#             h2         = h[m1]\n",
    "#             conf2      = conf[m1]\n",
    "\n",
    "#             # -----------------------------\n",
    "#             # Step 2: Exact polygon mask\n",
    "#             # -----------------------------\n",
    "#             m2 = contains(AOI, lon2, lat2)\n",
    "#             if m2.sum() == 0:\n",
    "#                 continue\n",
    "\n",
    "#             # -----------------------------\n",
    "#             # Step 3: Build GeoDataFrame\n",
    "#             # -----------------------------\n",
    "#             gdf = gpd.GeoDataFrame(\n",
    "#                 {\n",
    "#                     \"latitude\": lat2[m2],\n",
    "#                     \"longitude\": lon2[m2],\n",
    "#                     \"h_ph\": h2[m2],\n",
    "#                     \"signal_conf\": list(conf2[m2]),\n",
    "#                     \"beam\": beam,\n",
    "#                     \"granule\": granule_name\n",
    "#                 },\n",
    "#                 geometry=[Point(x, y) for x, y in zip(lon2[m2], lat2[m2])],\n",
    "#                 crs=\"EPSG:4326\"\n",
    "#             )\n",
    "\n",
    "#             # -----------------------------\n",
    "#             # Step 4: Save .parquet\n",
    "#             # -----------------------------\n",
    "#             parquet_name = f\"{granule_name}_{beam}.parquet\"\n",
    "#             outfile_parquet = CLIPPED_PARQUET_DIR / parquet_name\n",
    "#             gdf.to_parquet(outfile_parquet)\n",
    "#             print(f\"   Saved Parquet: {parquet_name} ({len(gdf)} photons)\")\n",
    "\n",
    "#             # -----------------------------\n",
    "#             # Step 5: Save simplified .h5\n",
    "#             # -----------------------------\n",
    "#             save_to_h5(gdf, granule_name, beam)\n",
    "\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 7) Run clipping over all files\n",
    "# # ------------------------------------------------------------\n",
    "# for f in files:\n",
    "#     clip_one(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39bf48a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbendopa\\AppData\\Local\\Temp\\ipykernel_13560\\1094105240.py:21: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  AOI = aoi_gdf.unary_union\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOI loaded. Bounds: (-153.93582877221303, 70.872503521221, -153.673676720986, 70.89384248884699)\n",
      "Found 2 raw ATL03 granules\n",
      "\n",
      "Processing: ATL03_20190407054915_01370305_007_01\n",
      "  Beam gt2l: 13721 photons\n",
      "  Beam gt2r: 4339 photons\n",
      "  Beam gt3l: 16666 photons\n",
      "  Beam gt3r: 4541 photons\n",
      "Saved: ATL03_20190407054915_01370305_007_01_clipped.h5\n",
      "Saved merged parquet\n",
      "\n",
      "Processing: ATL03_20241225020311_01372605_007_01\n",
      "  Beam gt2l: 10380 photons\n",
      "  Beam gt2r: 3069 photons\n",
      "  Beam gt3l: 13016 photons\n",
      "  Beam gt3r: 3240 photons\n",
      "Saved: ATL03_20241225020311_01372605_007_01_clipped.h5\n",
      "Saved merged parquet\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ATL03 CLIPPING — ONE CLEAN HDF5 PER GRANULE WITH ALL 6 BEAMS\n",
    "# ============================================================\n",
    "\n",
    "import pathlib as pl\n",
    "import h5py\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.vectorized import contains\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load AOI shapefile (this ensures AOI is correctly defined)\n",
    "# ------------------------------------------------------------\n",
    "AOI_SHP = r\"C:\\coding\\arctic\\paper1\\shp\\AOI_Drew_CLEAN.shp\"\n",
    "aoi_gdf = gpd.read_file(AOI_SHP)\n",
    "if aoi_gdf.crs.to_epsg() != 4326:\n",
    "    aoi_gdf = aoi_gdf.to_crs(4326)\n",
    "AOI = aoi_gdf.unary_union\n",
    "\n",
    "minx, miny, maxx, maxy = AOI.bounds\n",
    "print(\"AOI loaded. Bounds:\", AOI.bounds)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Input raw ATL03 granules\n",
    "# ------------------------------------------------------------\n",
    "RAW_DIR = pl.Path(r\"C:\\coding\\arctic\\paper1\\ATL06_all\\ATL03\\0129\\0137\")\n",
    "files = list(RAW_DIR.glob(\"*.h5\"))\n",
    "print(\"Found\", len(files), \"raw ATL03 granules\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Output directory for clipped results\n",
    "# ------------------------------------------------------------\n",
    "OUT_DIR = pl.Path(r\"C:\\coding\\arctic\\paper1\\ATL06_all\\ATL03\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Beams\n",
    "# ------------------------------------------------------------\n",
    "BEAMS = [\"gt1l\",\"gt1r\",\"gt2l\",\"gt2r\",\"gt3l\",\"gt3r\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Clip a single granule and save ONE multi-beam HDF5\n",
    "# ------------------------------------------------------------\n",
    "def clip_granule(path):\n",
    "    granule = path.stem\n",
    "    print(f\"\\nProcessing: {granule}\")\n",
    "\n",
    "    # create one output h5 file for all beams\n",
    "    out_h5 = OUT_DIR / f\"{granule}_clipped.h5\"\n",
    "    h5_out = h5py.File(out_h5, \"w\")\n",
    "\n",
    "    # optional: merged parquet with all beams\n",
    "    merged_list = []\n",
    "\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "\n",
    "        for beam in BEAMS:\n",
    "\n",
    "            grp = f.get(f\"{beam}/heights\")\n",
    "            if grp is None:\n",
    "                continue\n",
    "\n",
    "            lat  = grp[\"lat_ph\"][:]\n",
    "            lon  = grp[\"lon_ph\"][:]\n",
    "            h    = grp[\"h_ph\"][:]\n",
    "            conf = grp[\"signal_conf_ph\"][:]\n",
    "\n",
    "            # ---- Step 1: Fast bounding box filter ----\n",
    "            bb_mask = (\n",
    "                (lon >= minx) & (lon <= maxx) &\n",
    "                (lat >= miny) & (lat <= maxy)\n",
    "            )\n",
    "            if bb_mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            lat2 = lat[bb_mask]\n",
    "            lon2 = lon[bb_mask]\n",
    "            h2   = h[bb_mask]\n",
    "            conf2 = conf[bb_mask]\n",
    "\n",
    "            # ---- Step 2: Exact AOI polygon filter ----\n",
    "            poly_mask = contains(AOI, lon2, lat2)\n",
    "\n",
    "            if poly_mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # Final clipped data\n",
    "            lat3  = lat2[poly_mask]\n",
    "            lon3  = lon2[poly_mask]\n",
    "            h3    = h2[poly_mask]\n",
    "            conf3 = conf2[poly_mask]\n",
    "\n",
    "            print(f\"  Beam {beam}: {len(lat3)} photons\")\n",
    "\n",
    "            # ---- Save inside the multi-beam H5 ----\n",
    "            bgrp = h5_out.create_group(beam)\n",
    "            bgrp.create_dataset(\"lat_ph\", data=lat3)\n",
    "            bgrp.create_dataset(\"lon_ph\", data=lon3)\n",
    "            bgrp.create_dataset(\"h_ph\", data=h3)\n",
    "            bgrp.create_dataset(\"signal_conf_ph\", data=conf3)\n",
    "\n",
    "            # ---- Save for optional parquet merging ----\n",
    "            merged_list.append(\n",
    "                gpd.GeoDataFrame(\n",
    "                    {\n",
    "                        \"latitude\": lat3,\n",
    "                        \"longitude\": lon3,\n",
    "                        \"h_ph\": h3,\n",
    "                        \"beam\": beam,\n",
    "                        \"granule\": granule,\n",
    "                        \"signal_conf\": list(conf3),\n",
    "                    },\n",
    "                    geometry=[Point(x, y) for x,y in zip(lon3, lat3)],\n",
    "                    crs=\"EPSG:4326\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    h5_out.close()\n",
    "    print(\"Saved:\", out_h5.name)\n",
    "\n",
    "    # -------- Optional merged parquet --------\n",
    "    if len(merged_list) > 0:\n",
    "        merged = gpd.GeoDataFrame( pd.concat(merged_list, ignore_index=True), crs=\"EPSG:4326\")\n",
    "        merged.to_parquet(OUT_DIR / f\"{granule}_clipped.parquet\")\n",
    "        print(\"Saved merged parquet\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Run clipping for all granules\n",
    "# ------------------------------------------------------------\n",
    "for f in files:\n",
    "    clip_granule(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e4bc2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>h_ph</th>\n",
       "      <th>signal_conf</th>\n",
       "      <th>beam</th>\n",
       "      <th>granule</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-153.895411</td>\n",
       "      <td>70.787374</td>\n",
       "      <td>2.876066</td>\n",
       "      <td>[4, 4, 4, 4, 4]</td>\n",
       "      <td>gt1l</td>\n",
       "      <td>ATL03_20190105212430_01290203_007_01.h5</td>\n",
       "      <td>POINT (-153.89541 70.78737)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-153.895411</td>\n",
       "      <td>70.787374</td>\n",
       "      <td>2.884471</td>\n",
       "      <td>[4, 4, 4, 4, 4]</td>\n",
       "      <td>gt1l</td>\n",
       "      <td>ATL03_20190105212430_01290203_007_01.h5</td>\n",
       "      <td>POINT (-153.89541 70.78737)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-153.895411</td>\n",
       "      <td>70.787374</td>\n",
       "      <td>3.036654</td>\n",
       "      <td>[4, 4, 4, 4, 4]</td>\n",
       "      <td>gt1l</td>\n",
       "      <td>ATL03_20190105212430_01290203_007_01.h5</td>\n",
       "      <td>POINT (-153.89541 70.78737)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-153.895411</td>\n",
       "      <td>70.787374</td>\n",
       "      <td>2.988105</td>\n",
       "      <td>[4, 4, 4, 4, 4]</td>\n",
       "      <td>gt1l</td>\n",
       "      <td>ATL03_20190105212430_01290203_007_01.h5</td>\n",
       "      <td>POINT (-153.89541 70.78737)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-153.895411</td>\n",
       "      <td>70.787374</td>\n",
       "      <td>2.942522</td>\n",
       "      <td>[4, 4, 4, 4, 4]</td>\n",
       "      <td>gt1l</td>\n",
       "      <td>ATL03_20190105212430_01290203_007_01.h5</td>\n",
       "      <td>POINT (-153.89541 70.78737)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    longitude   latitude      h_ph      signal_conf  beam  \\\n",
       "0 -153.895411  70.787374  2.876066  [4, 4, 4, 4, 4]  gt1l   \n",
       "1 -153.895411  70.787374  2.884471  [4, 4, 4, 4, 4]  gt1l   \n",
       "2 -153.895411  70.787374  3.036654  [4, 4, 4, 4, 4]  gt1l   \n",
       "3 -153.895411  70.787374  2.988105  [4, 4, 4, 4, 4]  gt1l   \n",
       "4 -153.895411  70.787374  2.942522  [4, 4, 4, 4, 4]  gt1l   \n",
       "\n",
       "                                   granule                     geometry  \n",
       "0  ATL03_20190105212430_01290203_007_01.h5  POINT (-153.89541 70.78737)  \n",
       "1  ATL03_20190105212430_01290203_007_01.h5  POINT (-153.89541 70.78737)  \n",
       "2  ATL03_20190105212430_01290203_007_01.h5  POINT (-153.89541 70.78737)  \n",
       "3  ATL03_20190105212430_01290203_007_01.h5  POINT (-153.89541 70.78737)  \n",
       "4  ATL03_20190105212430_01290203_007_01.h5  POINT (-153.89541 70.78737)  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "\n",
    "CLIPPED_DIR = pl.Path(r\"C:\\coding\\arctic\\paper1\\ATL06_all\\ATL03_clipped\")\n",
    "\n",
    "# Load all clipped beams for profile analysis\n",
    "all_gdfs = []\n",
    "for p in CLIPPED_DIR.glob(\"*.parquet\"):\n",
    "    gdf = gpd.read_parquet(p)\n",
    "    all_gdfs.append(gdf)\n",
    "\n",
    "atl03_clipped = gpd.GeoDataFrame(pd.concat(all_gdfs, ignore_index=True), crs=\"EPSG:4326\")\n",
    "\n",
    "atl03_clipped.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab635c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
